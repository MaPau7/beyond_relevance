{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "291b3994-62d2-4181-9e24-5b6e7c7fdc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mcortes/pir/full_dataset/full_run\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c31cd1b-8e6b-4936-a281-35bb0625b9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever module loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import Retriever\n",
    "print(\"Retriever module loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19a25590-2b1b-436c-91be-1124abd911c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever.similarity module loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import Retriever.similarity\n",
    "print(\"Retriever.similarity module loaded successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da31ade2-aa20-4ce6-9cbd-0ac184759e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(\"/home/mcortes/pir/full_dataset/full_run/Retriever/similarity/dpr.ipynb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2cde932-e9c6-42b2-b108-9099f44ce590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever found: True\n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "retriever_spec = importlib.util.find_spec(\"Retriever\")\n",
    "print(\"Retriever found:\", retriever_spec is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e16a3557-43ad-4bcc-8d67-9cfe0a746493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Retriever', 'main.ipynb', '.ipynb_checkpoints']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"/home/mcortes/pir/full_dataset/full_run\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b56372f3-eb57-4dd4-b3d2-41855800a5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(os.path.exists(\"/home/mcortes/pir/full_dataset/full_run/Retriever/__init__.py\"))\n",
    "print(os.path.exists(\"/home/mcortes/pir/full_dataset/full_run/Retriever/similarity/__init__.py\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb989250-2c3b-4511-a5bd-fab5fe37f299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated sys.path: ['/home/mcortes/pir/full_dataset/full_run', '/home/mcortes/.conda/envs/pir/lib/python313.zip', '/home/mcortes/.conda/envs/pir/lib/python3.13', '/home/mcortes/.conda/envs/pir/lib/python3.13/lib-dynload', '', '/home/mcortes/.conda/envs/pir/lib/python3.13/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "repo_path = \"/home/mcortes/pir/full_dataset/full_run\"  # Adjusted path\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.insert(0, repo_path)\n",
    "\n",
    "print(\"Updated sys.path:\", sys.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af0ea449-59fa-4371-9425-f4e0d4ad6fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "repo_path = \"/home/mcortes/pir/full_dataset/full_run\"  # Change this to match your actual Repo path\n",
    "sys.path.insert(0, repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6ad8c4-9a4d-4da6-b050-7c32c50ab3d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aefc3e3-9844-40d4-811a-520f874829ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever/__init__.py: Exists\n",
      "Retriever/similarity/__init__.py: Exists\n",
      "Retriever/diversity/__init__.py: Exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folders = [\"Retriever\", \"Retriever/similarity\", \"Retriever/diversity\"]\n",
    "for folder in folders:\n",
    "    init_file = os.path.join(folder, \"__init__.py\")\n",
    "    print(f\"{init_file}: {'Exists' if os.path.exists(init_file) else 'Missing'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "723673a6-4370-4cff-8010-3d989377c8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/mcortes/pir/full_dataset/full_run', '/home/mcortes/.conda/envs/pir/lib/python313.zip', '/home/mcortes/.conda/envs/pir/lib/python3.13', '/home/mcortes/.conda/envs/pir/lib/python3.13/lib-dynload', '', '/home/mcortes/.conda/envs/pir/lib/python3.13/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2145d05-e0d9-4f82-af66-e49ae106e9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "from Retriever.similarity.dpr import DPRRetriever\n",
    "from Retriever.similarity.contriever import ContrieverRetriever\n",
    "\n",
    "# datasets = {...}\n",
    "\n",
    "# retriever = DPRRetriever(\"dpr\")\n",
    "# retriever.retrieve(datasets)\n",
    "\n",
    "# retriever = ContrieverRetriever(\"contriever\")\n",
    "# retriever.retrieve(datasets)\n",
    "\n",
    "print(\"hi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e4d5330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "\n",
    "import openai\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy import stats\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer,T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer,DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "\n",
    "import logging\n",
    "import transformers\n",
    "transformers.tokenization_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.configuration_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.modeling_utils.logger.setLevel(logging.ERROR)\n",
    "\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "\n",
    "##########################################################Code for full datasets######################################################################\n",
    "\n",
    "# import json\n",
    "from collections import defaultdict,Counter\n",
    "\n",
    "import pandas as pd\n",
    "# import numpy as np\n",
    "# from tqdm.notebook import trange, tqdm\n",
    "# from scipy.spatial.distance import cosine\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00dc75af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset...\n",
      "agnews\n",
      "Query size and length 3674 89.36554164398476\n",
      "Corpus size and length 1051 169.64509990485251\n"
     ]
    }
   ],
   "source": [
    "# path = \"./demo_pir_dataset.json\"\n",
    "    \n",
    "# with open(path,\"r\",encoding=\"utf-8\") as f:\n",
    "#     datasets = json.load(f)\n",
    "\n",
    "# for k,v in datasets.items():\n",
    "#     print(k)\n",
    "#     q_len = [len(x.split()) for x in v[\"queries\"]]\n",
    "#     c_len = [len(x.split()) for x in v[\"corpus\"]]\n",
    "\n",
    "#     print(\"Query size and length\",len(v[\"queries\"]),sum(q_len)/len(q_len))\n",
    "#     print(\"Corpus size and length\",len(v[\"corpus\"]),sum(c_len)/len(c_len))\n",
    "\n",
    "##########################################################Code for full datasets######################################################################\n",
    "\n",
    "task = \"agnews\" # options = [\"ambigqa\",\"story\",\"agnews\",\"allsides\",\"perspectrum\",\"exfever\"]\n",
    "\n",
    "qrels_df = pd.read_json(\"hf://datasets/trumancai/perspective-information-retrieval-\"+task+\"/qrels/test.jsonl\", lines=True)\n",
    "queries_df = pd.read_json(\"hf://datasets/trumancai/perspective-information-retrieval-\"+task+\"/queries.jsonl\", lines=True)\n",
    "corpus_df = pd.read_json(\"hf://datasets/trumancai/perspective-information-retrieval-\"+task+\"/corpus.jsonl\", lines=True)\n",
    "\n",
    "qrels = qrels_df.to_dict('records')\n",
    "queries = queries_df.to_dict('records')\n",
    "corpus = corpus_df.to_dict('records')\n",
    "\n",
    "qid2ind,cid2ind = {},{}\n",
    "for i, q in enumerate(queries):\n",
    "  qid2ind[q[\"_id\"]] = str(i)\n",
    "\n",
    "for i, c in enumerate(corpus):\n",
    "  cid2ind[c[\"_id\"]] = str(i)\n",
    "\n",
    "# convert the hugginface dataset back to the original format\n",
    "datasets={}\n",
    "datasets[task] = {\"queries\":[],\"source_queries\":[],\"perspectives\":[],\"corpus\":[],\"key_ref\":{},\"query_labels\":[]}\n",
    "\n",
    "for q in queries:\n",
    "  datasets[task][\"queries\"].append(q[\"text\"])\n",
    "  meta = json.loads(q[\"meta\"])\n",
    "  datasets[task][\"source_queries\"].append(meta[\"src_query\"])\n",
    "  datasets[task][\"perspectives\"].append(meta[\"perspective\"])\n",
    "\n",
    "  temp_ref = {\"none\":\"none\"}\n",
    "  q_label = \"none\"\n",
    "  if task == \"ambigqa\":\n",
    "    q_label = \"perspective\"\n",
    "    temp_ref = {\"perspective\":\"perspective\"}\n",
    "\n",
    "  if task == \"story\":\n",
    "    q_label = \"analogy\"\n",
    "    temp_ref = {\"entities\":\"entity\", \"analogy\":\"analogy\"}\n",
    "\n",
    "  if task == \"agnews\":\n",
    "    q_label = \"subtopic\"\n",
    "    temp_ref = {\"relates to\":\"subtopic\", \"happened in\":\"location\"}\n",
    "\n",
    "  if task == \"allsides\":\n",
    "    q_label = \"left\"\n",
    "    temp_ref = {\"left\":\"left\", \"right\":\"right\", \"center\":\"center\"}\n",
    "\n",
    "  if task == \"perspectrum\":\n",
    "    q_label = \"support\"\n",
    "    temp_ref = {\"support\":\"support\", \"oppose\":\"undermine\", \"relates to\":\"general\"}\n",
    "\n",
    "  if task == \"exfever\":\n",
    "    q_label = \"NOT ENOUGH INFO\"\n",
    "    temp_ref = {\"supports\":\"SUPPORT\", \"refutes\":\"REFUTE\", \"no information\":\"NOT ENOUGH INFO\"}\n",
    "\n",
    "  for k in temp_ref.keys():\n",
    "    if k in meta[\"perspective\"]:\n",
    "      q_label = temp_ref[k]\n",
    "\n",
    "  datasets[task][\"query_labels\"].append(q_label)\n",
    "\n",
    "for c in corpus:\n",
    "  datasets[task][\"corpus\"].append(c[\"text\"])\n",
    "\n",
    "\n",
    "datasets[task][\"key_ref\"] =  defaultdict(list)\n",
    "\n",
    "for rel in qrels:\n",
    "  if rel[\"score\"]== 1:\n",
    "    datasets[task][\"key_ref\"][qid2ind[rel[\"query-id\"]]].append(cid2ind[rel[\"corpus-id\"]])\n",
    "\n",
    "\n",
    "print(\"original dataset...\")\n",
    "\n",
    "for k,v in datasets.items():\n",
    "    print(k)\n",
    "    q_len = [len(x.split()) for x in v[\"queries\"]]\n",
    "    c_len = [len(x.split()) for x in v[\"corpus\"]]\n",
    "\n",
    "    print(\"Query size and length\",len(v[\"queries\"]),sum(q_len)/len(q_len))\n",
    "    print(\"Corpus size and length\",len(v[\"corpus\"]),sum(c_len)/len(c_len))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f57abd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_agnews\n",
      "1837\n",
      "Query size and length 1837 75.0647795318454\n",
      "Corpus size and length 1051 169.64509990485251\n"
     ]
    }
   ],
   "source": [
    "# create a root query only dataset\n",
    "source_datasets = {}\n",
    "\n",
    "for data_name, dataset in datasets.items():\n",
    "    # {\"queries\":[],\"source_queries\":[],\"perspectives\":[],\"corpus\":[],\"key_ref\":{},\"query_labels\":[]}\n",
    "    source_datasets[\"source_\"+data_name] = {\"corpus\":dataset[\"corpus\"],\"queries\":[],\"source_queries\":[],\"perspectives\":[],\"key_ref\":{},\"query_labels\":[]}\n",
    "    \n",
    "    reverse_source_query_dic = {}\n",
    "    \n",
    "    for i, query in enumerate(dataset[\"source_queries\"]):\n",
    "        if query not in list(reverse_source_query_dic.keys()):\n",
    "            query_id = str(len(source_datasets[\"source_\"+data_name][\"queries\"]))\n",
    "            reverse_source_query_dic[query] = query_id\n",
    "            source_datasets[\"source_\"+data_name][\"queries\"].append(query)\n",
    "            source_datasets[\"source_\"+data_name][\"source_queries\"].append(query)\n",
    "            source_datasets[\"source_\"+data_name][\"perspectives\"].append(\"none\")\n",
    "            source_datasets[\"source_\"+data_name][\"query_labels\"].append(\"none\")\n",
    "            source_datasets[\"source_\"+data_name][\"key_ref\"][query_id] = dataset[\"key_ref\"][str(i)]\n",
    "        else:\n",
    "            # this source query already exists\n",
    "            source_datasets[\"source_\"+data_name][\"key_ref\"][str(reverse_source_query_dic[query])].extend(dataset[\"key_ref\"][str(i)])\n",
    "\n",
    "        \n",
    "for k,v in source_datasets.items():\n",
    "    print(k)\n",
    "    print(len(v[\"key_ref\"].keys()))\n",
    "    \n",
    "    q_len = [len(x.split()) for x in v[\"queries\"]]\n",
    "    c_len = [len(x.split()) for x in v[\"corpus\"]]\n",
    "\n",
    "    print(\"Query size and length\",len(v[\"queries\"]),sum(q_len)/len(q_len))\n",
    "    print(\"Corpus size and length\",len(v[\"corpus\"]),sum(c_len)/len(c_len))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "199f9893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 417\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# evaluate on NON-source datasets\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# def evaluation(key_ref, corpus_scores, query_labels, dataset_name):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# retriever = ContrieverRetriever(\"contriever\")\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# retriever.retrieve(datasets)\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m retriever \u001b[38;5;241m=\u001b[39m \u001b[43mDPRRetriever\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdpr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m retriever\u001b[38;5;241m.\u001b[39mretrieve(datasets)\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# model_names = [\"dpr\", \"simcse-unsup\", \"simcse-sup\"]\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;66;03m# for model_name in model_names:\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;66;03m#     print(\"============\", model_name, \"============\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# print(\"Contriever\")\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# contriever_main(datasets)\u001b[39;00m\n",
      "File \u001b[0;32m~/pir/full_dataset/full_run/Retriever/base.py:19\u001b[0m, in \u001b[0;36mBaseRetriever.__init__\u001b[0;34m(self, model_name)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_model()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# evaluate on NON-source datasets\n",
    "\n",
    "# def evaluation(key_ref, corpus_scores, query_labels, dataset_name):\n",
    "#     # evaluation of a dataset    \n",
    "#     recall_threshold = [1,5,10]\n",
    "#     recall_results = [0 for thresh in recall_threshold]\n",
    "    \n",
    "#     if \"source\" in dataset_name:\n",
    "#         parts = [\"none\"]\n",
    "#     else:\n",
    "#         if dataset_name == \"perspectrum\":\n",
    "#             parts = [\"support\",\"undermine\",\"general\"]\n",
    "#         elif dataset_name == \"agnews\":\n",
    "#             parts = [\"subtopic\", \"location\"]\n",
    "#         elif dataset_name == \"story\":\n",
    "#             parts = [\"analogy\", \"entity\"]\n",
    "#         elif dataset_name == \"ambigqa\":\n",
    "#             parts = [\"perspective\"]\n",
    "#         elif dataset_name == \"allsides\":\n",
    "#             parts = [\"left\",\"right\",\"center\"]\n",
    "#         elif dataset_name == \"exfever\":\n",
    "#             parts = [\"SUPPORT\",\"REFUTE\",\"NOT ENOUGH INFO\"]\n",
    "    \n",
    "#     parts_size = [0 for x in parts]\n",
    "        \n",
    "#     for lb in query_labels:\n",
    "#         parts_size[parts.index(lb)] += 1\n",
    "            \n",
    "#     partial_recall_results = []\n",
    "#     for i in range(len(parts)):\n",
    "#         partial_recall_results.append([0 for thresh in recall_threshold])\n",
    "\n",
    "    \n",
    "#     for k,v in key_ref.items():\n",
    "#         for j, thresh in enumerate(recall_threshold):\n",
    "#             # important: find one is ok, this can be modified\n",
    "#             ranked_scores = (-np.array(corpus_scores[int(k)])).argsort()[:thresh]\n",
    "            \n",
    "            \n",
    "#             indicator = 0\n",
    "#             try:\n",
    "#                 for index in v:\n",
    "#                     if index in ranked_scores:\n",
    "#                         indicator = 1 \n",
    "#             except:\n",
    "#                 for index in [v]:\n",
    "#                     if index in ranked_scores:\n",
    "#                         indicator = 1                \n",
    "#             recall_results[j] += indicator\n",
    "#             partial_recall_results[parts.index(query_labels[int(k)])][j] += indicator\n",
    "    \n",
    "#     final_results = [result/len(key_ref.items()) for result in recall_results]\n",
    "        \n",
    "#     print(\"overall\")\n",
    "#     for i, thresh in enumerate(recall_threshold):\n",
    "#         print(\"Recall@\"+str(thresh)+\":\",final_results[i])\n",
    "        \n",
    "#     macro_threshs = [[] for x in recall_threshold]\n",
    "    \n",
    "#     for t, recall_results in enumerate(partial_recall_results):\n",
    "#         print(parts[t])\n",
    "#         final_results = [result/parts_size[t] for result in recall_results]\n",
    "        \n",
    "#         for i, thresh in enumerate(recall_threshold):\n",
    "#             print(\"Recall@\"+str(thresh)+\":\",final_results[i])\n",
    "#             macro_threshs[i].append(final_results[i])\n",
    "                \n",
    "#     print(\"macro_average\")\n",
    "#     for i, thresh in enumerate(recall_threshold):\n",
    "#         print(\"Recall@\"+str(thresh)+\":\",sum(macro_threshs[i])/len(macro_threshs[i]))\n",
    "\n",
    "\n",
    "##########################################################Code for full datasets######################################################################\n",
    "\n",
    "# def evaluation(key_ref, corpus_scores, query_labels, dataset_name):\n",
    "#     # evaluation of a dataset    \n",
    "#     recall_threshold = [1,5,10]\n",
    "#     recall_results = [0 for thresh in recall_threshold]\n",
    "    \n",
    "#     if \"source\" in dataset_name:\n",
    "#         parts = [\"none\"]\n",
    "#     else:\n",
    "#         if dataset_name == \"perspectrum\":\n",
    "#             parts = [\"support\",\"undermine\",\"general\"]\n",
    "#         elif dataset_name == \"agnews\":\n",
    "#             parts = [\"subtopic\", \"location\"]\n",
    "#         elif dataset_name == \"story\":\n",
    "#             parts = [\"analogy\", \"entity\"]\n",
    "#         elif dataset_name == \"ambigqa\":\n",
    "#             parts = [\"perspective\"]\n",
    "#         elif dataset_name == \"allsides\":\n",
    "#             parts = [\"left\",\"right\",\"center\"]\n",
    "#         elif dataset_name == \"exfever\":\n",
    "#             parts = [\"SUPPORT\",\"REFUTE\",\"NOT ENOUGH INFO\"]\n",
    "    \n",
    "#     parts_size = [0 for x in parts]\n",
    "        \n",
    "#     for lb in query_labels:\n",
    "#         parts_size[parts.index(lb)] += 1\n",
    "            \n",
    "#     partial_recall_results = []\n",
    "#     for i in range(len(parts)):\n",
    "#         partial_recall_results.append([0 for thresh in recall_threshold])\n",
    "\n",
    "    \n",
    "#     for k,v in key_ref.items():\n",
    "#         for j, thresh in enumerate(recall_threshold):\n",
    "#             # important: find one is ok, this can be modified\n",
    "#             ranked_scores = (-np.array(corpus_scores[int(k)])).argsort()[:thresh]\n",
    "            \n",
    "#             indicator = 0\n",
    "#             try:\n",
    "#                 for index in v:\n",
    "#                     if int(index) in ranked_scores:\n",
    "#                         indicator = 1 \n",
    "#             except:\n",
    "#                 for index in [v]:\n",
    "#                     if int(index) in ranked_scores:\n",
    "#                         indicator = 1                \n",
    "#             recall_results[j] += indicator\n",
    "#             partial_recall_results[parts.index(query_labels[int(k)])][j] += indicator\n",
    "    \n",
    "#     final_results = [result/len(key_ref.items()) for result in recall_results]\n",
    "        \n",
    "#     print(\"overall\",end=\": \")\n",
    "#     for i, thresh in enumerate(recall_threshold):\n",
    "#         print(\"Recall@\"+str(thresh)+\":\",round(final_results[i],3),end=\"; \")\n",
    "        \n",
    "#     macro_threshs = [[] for x in recall_threshold]\n",
    "#     print()\n",
    "#     for t, recall_results in enumerate(partial_recall_results):\n",
    "#         print(parts[t],end=\": \")\n",
    "#         final_results = [result/parts_size[t] for result in recall_results]\n",
    "        \n",
    "#         for i, thresh in enumerate(recall_threshold):\n",
    "#             print(\"Recall@\"+str(thresh)+\":\",round(final_results[i],3),end=\"; \")\n",
    "#             macro_threshs[i].append(final_results[i])\n",
    "#         print()\n",
    "                \n",
    "                    \n",
    "\n",
    "# # # BM25 and BERTScore\n",
    "# # from rank_bm25 import BM25Okapi\n",
    "# from evaluate import load\n",
    "\n",
    "\n",
    "# import logging\n",
    "# import transformers\n",
    "# transformers.tokenization_utils.logger.setLevel(logging.ERROR)\n",
    "# transformers.configuration_utils.logger.setLevel(logging.ERROR)\n",
    "# transformers.modeling_utils.logger.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "# def bm25_main(datasets):\n",
    "#     # corpuses,key_refs = corpus_building(datasets)\n",
    "\n",
    "#     for k,v in datasets.items():\n",
    "#         print(\"we are working on:\",k)\n",
    "        \n",
    "#         queries = v[\"queries\"]\n",
    "#         corpus = v[\"corpus\"]\n",
    "#         key_ref = v[\"key_ref\"]\n",
    "#         query_labels = v[\"query_labels\"]\n",
    "        \n",
    "#         tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "#         bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "#         corpus_scores = []\n",
    "\n",
    "#         for query in tqdm(queries):\n",
    "#             # query = item[\"query\"]\n",
    "#             tokenized_query = query.split(\" \")\n",
    "#             doc_scores = bm25.get_scores(tokenized_query)\n",
    "#             corpus_scores.append(doc_scores)\n",
    "        \n",
    "#         with open(\"bm25_\"+k+\"_scores.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "#             json.dump([x.tolist() for x in corpus_scores],f)\n",
    "        \n",
    "#         evaluation(key_ref, corpus_scores, query_labels, k)\n",
    "#         print()\n",
    "        \n",
    "        \n",
    "# def extract_layer_cls(embeddings,layer):\n",
    "#     rep = []\n",
    "#     this_layer_embeddings = embeddings[layer]\n",
    "#     for emb in this_layer_embeddings:\n",
    "#         rep.append(emb[0])\n",
    "\n",
    "\n",
    "#     return rep\n",
    "\n",
    "        \n",
    "# def create_embeddings(tokenizer, model, texts):\n",
    "    \n",
    "#     if torch.cuda.is_available():\n",
    "#         device = torch.device('cuda')\n",
    "#     else:\n",
    "#         device = torch.device('cpu')\n",
    "\n",
    "#     print(device)\n",
    "        \n",
    "#     # create tokenized inputs\n",
    "#     batch_size = 17 #29\n",
    "\n",
    "#     model.to(device)\n",
    "\n",
    "#     # naive batching\n",
    "#     if len(texts) < batch_size:\n",
    "#         inputs = tokenizer(texts,max_length=80, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "#         inputs = inputs.to(device)\n",
    "#         with torch.no_grad():\n",
    "#             batch_embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output\n",
    "#             embeddings = []\n",
    "#             for embedding in batch_embeddings:\n",
    "#                 embeddings.append(embedding.detach().cpu().tolist())\n",
    "#             del batch_embeddings\n",
    "#             torch.cuda.empty_cache()\n",
    "\n",
    "#     else:\n",
    "#         embeddings = []\n",
    "#         # num_batch = len(texts)//batch_size\n",
    "\n",
    "#         # fixed this to handle cases when len of texts is a multiple of batch_size.\n",
    "#         num_batch = (len(texts) + batch_size - 1) // batch_size  # correct batch count\n",
    "\n",
    "#         for i in trange(num_batch):  # No extra iteration\n",
    "#             batch_start = i * batch_size\n",
    "#             batch_end = min(len(texts), (i + 1) * batch_size)\n",
    "#             batch_texts = texts[batch_start:batch_end]\n",
    "        \n",
    "#             if not batch_texts:  # Extra safety check\n",
    "#                 print('Batch is empty.')\n",
    "#                 continue\n",
    "\n",
    "                \n",
    "#         # embeddings = []\n",
    "#         # num_batch = len(texts)//batch_size\n",
    "\n",
    "#         # for i in trange(num_batch+1):\n",
    "#         #     batch_start = i*batch_size\n",
    "#         #     batch_end = min(len(texts), (i+1)*batch_size)\n",
    "#         #     batch_texts = texts[batch_start:batch_end]\n",
    "\n",
    "#             inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "#             inputs = inputs.to(device)\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 try:\n",
    "#                     batch_embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output\n",
    "#                     embeddings.extend(batch_embeddings.detach().cpu().tolist())\n",
    "\n",
    "#                     # save cuda memory\n",
    "#                     del batch_embeddings\n",
    "#                     del inputs\n",
    "#                     torch.cuda.empty_cache()\n",
    "#                 except:\n",
    "#                     message = \"broken embeddings\"\n",
    "\n",
    "#     # 25 * num_example * seq_len * 768 -> num_example * 768\n",
    "#     return embeddings\n",
    "        \n",
    "    \n",
    "# def dpr_main(datasets, model_name):\n",
    "#     # corpuses,key_refs = corpus_building(datasets)\n",
    "\n",
    "#     if model_name == \"dpr\":\n",
    "#         ctokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "#         cmodel = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "#         qtokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "#         qmodel = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "        \n",
    "#     elif model_name in [\"simcse-unsup\",\"simcse-sup\"]:\n",
    "#         model_mapping = {\n",
    "#             \"simcse-unsup\":\"princeton-nlp/unsup-simcse-bert-base-uncased\",\n",
    "#             \"simcse-sup\":\"princeton-nlp/sup-simcse-bert-base-uncased\",\n",
    "#         }\n",
    "        \n",
    "#         ctokenizer = AutoTokenizer.from_pretrained(model_mapping[model_name])\n",
    "#         cmodel = AutoModel.from_pretrained(model_mapping[model_name])\n",
    "#         qtokenizer = AutoTokenizer.from_pretrained(model_mapping[model_name])\n",
    "#         qmodel = AutoModel.from_pretrained(model_mapping[model_name])\n",
    "\n",
    "            \n",
    "#     qmodel.eval() \n",
    "#     cmodel.eval() \n",
    "    \n",
    "#     for k,v in datasets.items():\n",
    "        \n",
    "#         print(\"we are working on:\",k)\n",
    "#         corpus_scores = []\n",
    "        \n",
    "#         queries = v[\"queries\"]\n",
    "#         corpus = v[\"corpus\"]\n",
    "#         key_ref = v[\"key_ref\"]\n",
    "#         query_labels = v[\"query_labels\"]\n",
    "            \n",
    "#         if model_name in [\"t5\",\"flan-t5\",\"unifiedqa\"]:\n",
    "#             query_embeddings = create_T5_embeddings(qtokenizer, qmodel, queries,0)\n",
    "#             corpus_embeddings = create_T5_embeddings(ctokenizer, cmodel, corpus, 0)  \n",
    "#         else:\n",
    "#             query_embeddings = create_embeddings(qtokenizer, qmodel, queries)\n",
    "#             corpus_embeddings = create_embeddings(ctokenizer, cmodel, corpus)\n",
    "        \n",
    "#         for emb1 in tqdm(query_embeddings):\n",
    "#             scores = []\n",
    "#             for emb2 in corpus_embeddings:\n",
    "#                 scores.append(1 - cosine(emb1, emb2))\n",
    "\n",
    "#             corpus_scores.append(scores)\n",
    "            \n",
    "#         with open(model_name+\"_\"+k+\"_scores.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "#             json.dump(corpus_scores,f)\n",
    "            \n",
    "#         evaluation(key_ref, corpus_scores, query_labels, k)\n",
    "#         print()\n",
    "        \n",
    "        \n",
    "# def contriever_main(datasets):\n",
    "#     # corpuses,key_refs = corpus_building(datasets)\n",
    "    \n",
    "#     def mean_pooling(token_embeddings, mask):\n",
    "#         token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\n",
    "#         sentence_embeddings = token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]\n",
    "        \n",
    "#         return sentence_embeddings\n",
    "    \n",
    "#     def contriever_embeddings(texts, tokenizer, model):\n",
    "#         device = torch.device('cuda')\n",
    "#         # device = torch.device('cpu')\n",
    "#         # create tokenized inputs\n",
    "#         batch_size = 29\n",
    "\n",
    "#         model.to(device)\n",
    "#         embeddings = []\n",
    "#         # naive batching\n",
    "#         if len(texts) < batch_size:\n",
    "#             # FIXED, should be 'texts' instead of 'sentences' in the line below.\n",
    "#             inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "#             # FIXED, added .to(device) to make sure input is on the GPU\n",
    "#             inputs = inputs.to(device)\n",
    "#             outputs = model(**inputs)    \n",
    "#             batch_embeddings = mean_pooling(outputs[0], inputs['attention_mask'])\n",
    "#             for embedding in batch_embeddings:\n",
    "#                 embeddings.append(embedding.detach().cpu().tolist())\n",
    "                \n",
    "#             del batch_embeddings\n",
    "#             torch.cuda.empty_cache()\n",
    "#         else:\n",
    "#             num_batch = len(texts)//batch_size\n",
    "\n",
    "#             for i in trange(num_batch+1):\n",
    "#                 batch_start = i*batch_size\n",
    "#                 batch_end = min(len(texts), (i+1)*batch_size)\n",
    "#                 batch_texts = texts[batch_start:batch_end]\n",
    "\n",
    "#                 inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "#                 inputs = inputs.to(device)\n",
    "\n",
    "#                 with torch.no_grad():\n",
    "#                     try:\n",
    "#                         batch_embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output\n",
    "#                         embeddings.extend(batch_embeddings.detach().cpu().tolist())\n",
    "#                         del batch_embeddings\n",
    "#                         del inputs\n",
    "#                         torch.cuda.empty_cache()\n",
    "#                     except:\n",
    "#                         message = \"broken embeddings\"   \n",
    "                        \n",
    "#         return embeddings\n",
    "    \n",
    "    \n",
    "# #     tokenizer = AutoTokenizer.from_pretrained('facebook/contriever-msmarco')\n",
    "# #     model = AutoModel.from_pretrained('facebook/contriever-msmarco')\n",
    "    \n",
    "#     tokenizer = AutoTokenizer.from_pretrained('facebook/contriever')\n",
    "#     model = AutoModel.from_pretrained('facebook/contriever')\n",
    "    \n",
    "    \n",
    "#     for k,v in datasets.items():\n",
    "        \n",
    "#         print(\"we are working on:\",k)\n",
    "#         corpus_scores = []\n",
    "        \n",
    "#         queries = v[\"queries\"]\n",
    "#         corpus = v[\"corpus\"]\n",
    "#         key_ref = v[\"key_ref\"]\n",
    "#         query_labels = v[\"query_labels\"]\n",
    "            \n",
    "#         query_embeddings = contriever_embeddings(queries, tokenizer, model)\n",
    "#         corpus_embeddings = contriever_embeddings(corpus, tokenizer, model)\n",
    "        \n",
    "#         for emb1 in tqdm(query_embeddings):\n",
    "#             scores = []\n",
    "#             for emb2 in corpus_embeddings:\n",
    "#                 scores.append(1 - cosine(emb1, emb2))\n",
    "\n",
    "#             corpus_scores.append(scores)\n",
    "            \n",
    "#         with open(\"contriver_\"+k+\"_scores.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "#             json.dump(corpus_scores,f)\n",
    "            \n",
    "#         evaluation(key_ref, corpus_scores, query_labels, k)\n",
    "#         print()\n",
    "        \n",
    "   \n",
    "# # print(\"BM25\")\n",
    "# # bm25_main(datasets)\n",
    "\n",
    "# # abs and aspire are not in the script\n",
    "# # model_names = [\"dpr\", \"simcse-unsup\", \"simcse-sup\",\"abs\",\"aspire\"]\n",
    "# retriever = DPRRetriever(\"dpr\")\n",
    "# retriever.retrieve(datasets)\n",
    "\n",
    "# retriever = ContrieverRetriever(\"contriever\")\n",
    "# retriever.retrieve(datasets)\n",
    "\n",
    "retriever = DPRRetriever(\"dpr\")\n",
    "retriever.retrieve(datasets)\n",
    "\n",
    "# model_names = [\"dpr\", \"simcse-unsup\", \"simcse-sup\"]\n",
    "# for model_name in model_names:\n",
    "#     print(\"============\", model_name, \"============\")\n",
    "#     dpr_main(datasets, model_name)\n",
    "\n",
    "# print(\"Contriever\")\n",
    "# contriever_main(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a718697-05df-49cd-87bb-60925be8de64",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Attempt to use the object\n",
    "    source_datasets\n",
    "    # If no exception is raised, the object exists\n",
    "    print(\"source datasets exists\")\n",
    "except NameError:\n",
    "    # If a NameError is raised, the object does not exist\n",
    "    print(\"source datasets does not exist\")\n",
    "\n",
    "for k,v in source_datasets.items():\n",
    "    print(k)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee58f5-56e8-48f7-b962-5fdf430b6f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on source datasets\n",
    "\n",
    "# def evaluation(key_ref, corpus_scores, query_labels, dataset_name):\n",
    "#     # evaluation of a dataset    \n",
    "#     recall_threshold = [1,5,10]\n",
    "#     recall_results = [0 for thresh in recall_threshold]\n",
    "    \n",
    "#     if \"source\" in dataset_name:\n",
    "#         parts = [\"none\"]\n",
    "#     else:\n",
    "#         if dataset_name == \"perspectrum\":\n",
    "#             parts = [\"support\",\"undermine\",\"general\"]\n",
    "#         elif dataset_name == \"agnews\":\n",
    "#             parts = [\"subtopic\", \"location\"]\n",
    "#         elif dataset_name == \"story\":\n",
    "#             parts = [\"analogy\", \"entity\"]\n",
    "#         elif dataset_name == \"ambigqa\":\n",
    "#             parts = [\"perspective\"]\n",
    "#         elif dataset_name == \"allsides\":\n",
    "#             parts = [\"left\",\"right\",\"center\"]\n",
    "#         elif dataset_name == \"exfever\":\n",
    "#             parts = [\"SUPPORT\",\"REFUTE\",\"NOT ENOUGH INFO\"]\n",
    "    \n",
    "#     parts_size = [0 for x in parts]\n",
    "        \n",
    "#     for lb in query_labels:\n",
    "#         parts_size[parts.index(lb)] += 1\n",
    "            \n",
    "#     partial_recall_results = []\n",
    "#     for i in range(len(parts)):\n",
    "#         partial_recall_results.append([0 for thresh in recall_threshold])\n",
    "\n",
    "    \n",
    "#     for k,v in key_ref.items():\n",
    "#         for j, thresh in enumerate(recall_threshold):\n",
    "#             # important: find one is ok, this can be modified\n",
    "#             ranked_scores = (-np.array(corpus_scores[int(k)])).argsort()[:thresh]\n",
    "            \n",
    "            \n",
    "#             indicator = 0\n",
    "#             try:\n",
    "#                 for index in v:\n",
    "#                     if index in ranked_scores:\n",
    "#                         indicator = 1 \n",
    "#             except:\n",
    "#                 for index in [v]:\n",
    "#                     if index in ranked_scores:\n",
    "#                         indicator = 1                \n",
    "#             recall_results[j] += indicator\n",
    "#             partial_recall_results[parts.index(query_labels[int(k)])][j] += indicator\n",
    "    \n",
    "#     final_results = [result/len(key_ref.items()) for result in recall_results]\n",
    "        \n",
    "#     print(\"overall\")\n",
    "#     for i, thresh in enumerate(recall_threshold):\n",
    "#         print(\"Recall@\"+str(thresh)+\":\",final_results[i])\n",
    "        \n",
    "#     macro_threshs = [[] for x in recall_threshold]\n",
    "    \n",
    "#     for t, recall_results in enumerate(partial_recall_results):\n",
    "#         print(parts[t])\n",
    "#         final_results = [result/parts_size[t] for result in recall_results]\n",
    "        \n",
    "#         for i, thresh in enumerate(recall_threshold):\n",
    "#             print(\"Recall@\"+str(thresh)+\":\",final_results[i])\n",
    "#             macro_threshs[i].append(final_results[i])\n",
    "                \n",
    "#     print(\"macro_average\")\n",
    "#     for i, thresh in enumerate(recall_threshold):\n",
    "#         print(\"Recall@\"+str(thresh)+\":\",sum(macro_threshs[i])/len(macro_threshs[i]))\n",
    "                \n",
    "\n",
    "##########################################################Code for full datasets######################################################################\n",
    "\n",
    "def evaluation(key_ref, corpus_scores, query_labels, dataset_name):\n",
    "    # evaluation of a dataset    \n",
    "    recall_threshold = [1,5,10]\n",
    "    recall_results = [0 for thresh in recall_threshold]\n",
    "    \n",
    "    if \"source\" in dataset_name:\n",
    "        parts = [\"none\"]\n",
    "    else:\n",
    "        if dataset_name == \"perspectrum\":\n",
    "            parts = [\"support\",\"undermine\",\"general\"]\n",
    "        elif dataset_name == \"agnews\":\n",
    "            parts = [\"subtopic\", \"location\"]\n",
    "        elif dataset_name == \"story\":\n",
    "            parts = [\"analogy\", \"entity\"]\n",
    "        elif dataset_name == \"ambigqa\":\n",
    "            parts = [\"perspective\"]\n",
    "        elif dataset_name == \"allsides\":\n",
    "            parts = [\"left\",\"right\",\"center\"]\n",
    "        elif dataset_name == \"exfever\":\n",
    "            parts = [\"SUPPORT\",\"REFUTE\",\"NOT ENOUGH INFO\"]\n",
    "    \n",
    "    parts_size = [0 for x in parts]\n",
    "        \n",
    "    for lb in query_labels:\n",
    "        parts_size[parts.index(lb)] += 1\n",
    "            \n",
    "    partial_recall_results = []\n",
    "    for i in range(len(parts)):\n",
    "        partial_recall_results.append([0 for thresh in recall_threshold])\n",
    "\n",
    "    \n",
    "    for k,v in key_ref.items():\n",
    "        for j, thresh in enumerate(recall_threshold):\n",
    "            # important: find one is ok, this can be modified\n",
    "            ranked_scores = (-np.array(corpus_scores[int(k)])).argsort()[:thresh]\n",
    "            \n",
    "            indicator = 0\n",
    "            try:\n",
    "                for index in v:\n",
    "                    if int(index) in ranked_scores:\n",
    "                        indicator = 1 \n",
    "            except:\n",
    "                for index in [v]:\n",
    "                    if int(index) in ranked_scores:\n",
    "                        indicator = 1                \n",
    "            recall_results[j] += indicator\n",
    "            partial_recall_results[parts.index(query_labels[int(k)])][j] += indicator\n",
    "    \n",
    "    final_results = [result/len(key_ref.items()) for result in recall_results]\n",
    "        \n",
    "    print(\"overall\",end=\": \")\n",
    "    for i, thresh in enumerate(recall_threshold):\n",
    "        print(\"Recall@\"+str(thresh)+\":\",round(final_results[i],3),end=\"; \")\n",
    "        \n",
    "    macro_threshs = [[] for x in recall_threshold]\n",
    "    print()\n",
    "    for t, recall_results in enumerate(partial_recall_results):\n",
    "        print(parts[t],end=\": \")\n",
    "        final_results = [result/parts_size[t] for result in recall_results]\n",
    "        \n",
    "        for i, thresh in enumerate(recall_threshold):\n",
    "            print(\"Recall@\"+str(thresh)+\":\",round(final_results[i],3),end=\"; \")\n",
    "            macro_threshs[i].append(final_results[i])\n",
    "        print()\n",
    "\n",
    "\n",
    "# BM25 and BERTScore\n",
    "from rank_bm25 import BM25Okapi\n",
    "from evaluate import load\n",
    "\n",
    "\n",
    "import logging\n",
    "import transformers\n",
    "transformers.tokenization_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.configuration_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.modeling_utils.logger.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "def bm25_main(datasets):\n",
    "    # corpuses,key_refs = corpus_building(datasets)\n",
    "\n",
    "    for k,v in datasets.items():\n",
    "        print(\"we are working on:\",k)\n",
    "        \n",
    "        queries = v[\"queries\"]\n",
    "        corpus = v[\"corpus\"]\n",
    "        key_ref = v[\"key_ref\"]\n",
    "        query_labels = v[\"query_labels\"]\n",
    "        \n",
    "        tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "        bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "        corpus_scores = []\n",
    "\n",
    "        for query in tqdm(queries):\n",
    "            # query = item[\"query\"]\n",
    "            tokenized_query = query.split(\" \")\n",
    "            doc_scores = bm25.get_scores(tokenized_query)\n",
    "            corpus_scores.append(doc_scores)\n",
    "        \n",
    "        with open(\"bm25_\"+k+\"_scores.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "            json.dump([x.tolist() for x in corpus_scores],f)\n",
    "        \n",
    "        evaluation(key_ref, corpus_scores, query_labels, k)\n",
    "        print()\n",
    "        \n",
    "        \n",
    "def extract_layer_cls(embeddings,layer):\n",
    "    rep = []\n",
    "    this_layer_embeddings = embeddings[layer]\n",
    "    for emb in this_layer_embeddings:\n",
    "        rep.append(emb[0])\n",
    "\n",
    "\n",
    "    return rep\n",
    "\n",
    "        \n",
    "def create_embeddings(tokenizer, model, texts):\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    print(device)\n",
    "        \n",
    "    # create tokenized inputs\n",
    "    batch_size = 17 #29\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # naive batching\n",
    "    if len(texts) < batch_size:\n",
    "        inputs = tokenizer(texts,max_length=80, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        inputs = inputs.to(device)\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output\n",
    "            embeddings = []\n",
    "            for embedding in batch_embeddings:\n",
    "                embeddings.append(embedding.detach().cpu().tolist())\n",
    "            del batch_embeddings\n",
    "            torch.cuda.empty_cache()\n",
    "    else:\n",
    "        embeddings = []\n",
    "        # num_batch = len(texts)//batch_size\n",
    "\n",
    "        # fixed this to handle cases when len of texts is a multiple of batch_size.\n",
    "        num_batch = (len(texts) + batch_size - 1) // batch_size  # correct batch count\n",
    "\n",
    "        for i in trange(num_batch):  # No extra iteration\n",
    "            batch_start = i * batch_size\n",
    "            batch_end = min(len(texts), (i + 1) * batch_size)\n",
    "            batch_texts = texts[batch_start:batch_end]\n",
    "        \n",
    "            if not batch_texts:  # Extra safety check\n",
    "                print('Batch is empty.')\n",
    "                continue\n",
    "        \n",
    "        # for i in trange(num_batch+1):\n",
    "        #     batch_start = i*batch_size\n",
    "        #     batch_end = min(len(texts), (i+1)*batch_size) # this is giving 17\n",
    "        #     print('len of texts is', len(texts), 'i+1 * batch_size is', ((i+1)*batch_size))\n",
    "        #     batch_texts = texts[batch_start:batch_end]\n",
    "            \n",
    "        #     print('start is ', batch_start, 'end is', batch_end)\n",
    "        #     print('len of texts is ', len(texts))\n",
    "        #     print('len of batch_texts is ', len(batch_texts))\n",
    "\n",
    "            inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    batch_embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output\n",
    "                    embeddings.extend(batch_embeddings.detach().cpu().tolist())\n",
    "\n",
    "                    # save cuda memory\n",
    "                    del batch_embeddings\n",
    "                    del inputs\n",
    "                    torch.cuda.empty_cache()\n",
    "                except:\n",
    "                    message = \"broken embeddings\"\n",
    "\n",
    "    # 25 * num_example * seq_len * 768 -> num_example * 768\n",
    "    return embeddings\n",
    "        \n",
    "    \n",
    "def dpr_main(datasets, model_name):\n",
    "    # corpuses,key_refs = corpus_building(datasets)\n",
    "\n",
    "    if model_name == \"dpr\":\n",
    "        ctokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "        cmodel = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "        qtokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "        qmodel = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "        \n",
    "    elif model_name in [\"simcse-unsup\",\"simcse-sup\"]:\n",
    "        model_mapping = {\n",
    "            \"simcse-unsup\":\"princeton-nlp/unsup-simcse-bert-base-uncased\",\n",
    "            \"simcse-sup\":\"princeton-nlp/sup-simcse-bert-base-uncased\",\n",
    "        }\n",
    "        \n",
    "        ctokenizer = AutoTokenizer.from_pretrained(model_mapping[model_name])\n",
    "        cmodel = AutoModel.from_pretrained(model_mapping[model_name])\n",
    "        qtokenizer = AutoTokenizer.from_pretrained(model_mapping[model_name])\n",
    "        qmodel = AutoModel.from_pretrained(model_mapping[model_name])\n",
    "\n",
    "            \n",
    "    qmodel.eval() \n",
    "    cmodel.eval() \n",
    "    \n",
    "    for k,v in datasets.items():\n",
    "        \n",
    "        print(\"we are working on:\",k)\n",
    "        corpus_scores = []\n",
    "        \n",
    "        queries = v[\"queries\"]\n",
    "        corpus = v[\"corpus\"]\n",
    "        key_ref = v[\"key_ref\"]\n",
    "        query_labels = v[\"query_labels\"]\n",
    "            \n",
    "        if model_name in [\"t5\",\"flan-t5\",\"unifiedqa\"]:\n",
    "            query_embeddings = create_T5_embeddings(qtokenizer, qmodel, queries,0)\n",
    "            corpus_embeddings = create_T5_embeddings(ctokenizer, cmodel, corpus, 0)  \n",
    "        else:\n",
    "            query_embeddings = create_embeddings(qtokenizer, qmodel, queries)\n",
    "            corpus_embeddings = create_embeddings(ctokenizer, cmodel, corpus)\n",
    "        \n",
    "        for emb1 in tqdm(query_embeddings):\n",
    "            scores = []\n",
    "            for emb2 in corpus_embeddings:\n",
    "                scores.append(1 - cosine(emb1, emb2))\n",
    "\n",
    "            corpus_scores.append(scores)\n",
    "            \n",
    "        with open(model_name+\"_\"+k+\"_scores.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "            json.dump(corpus_scores,f)\n",
    "            \n",
    "        evaluation(key_ref, corpus_scores, query_labels, k)\n",
    "        print()\n",
    "        \n",
    "        \n",
    "def contriever_main(datasets):\n",
    "    # corpuses,key_refs = corpus_building(datasets)\n",
    "    \n",
    "    def mean_pooling(token_embeddings, mask):\n",
    "        token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\n",
    "        sentence_embeddings = token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]\n",
    "        \n",
    "        return sentence_embeddings\n",
    "    \n",
    "    def contriever_embeddings(texts, tokenizer, model):\n",
    "        device = torch.device('cuda')\n",
    "        # device = torch.device('cpu')\n",
    "        # create tokenized inputs\n",
    "        batch_size = 29\n",
    "\n",
    "        model.to(device)\n",
    "        embeddings = []\n",
    "        # naive batching\n",
    "        if len(texts) < batch_size:\n",
    "            # FIXED, should be 'texts' instead of 'sentences' in the line below.\n",
    "            inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "            # FIXED, added .to(device) to make sure input is on the GPU\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(**inputs)\n",
    "            batch_embeddings = mean_pooling(outputs[0], inputs['attention_mask'])\n",
    "            for embedding in batch_embeddings:\n",
    "                embeddings.append(embedding.detach().cpu().tolist())\n",
    "                \n",
    "            del batch_embeddings\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            num_batch = len(texts)//batch_size\n",
    "\n",
    "            for i in trange(num_batch+1):\n",
    "                batch_start = i*batch_size\n",
    "                batch_end = min(len(texts), (i+1)*batch_size)\n",
    "                batch_texts = texts[batch_start:batch_end]\n",
    "\n",
    "                inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    try:\n",
    "                        batch_embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output\n",
    "                        embeddings.extend(batch_embeddings.detach().cpu().tolist())\n",
    "                        del batch_embeddings\n",
    "                        del inputs\n",
    "                        torch.cuda.empty_cache()\n",
    "                    except:\n",
    "                        message = \"broken embeddings\"   \n",
    "                        \n",
    "        return embeddings\n",
    "    \n",
    "    \n",
    "#     tokenizer = AutoTokenizer.from_pretrained('facebook/contriever-msmarco')\n",
    "#     model = AutoModel.from_pretrained('facebook/contriever-msmarco')\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained('facebook/contriever')\n",
    "    model = AutoModel.from_pretrained('facebook/contriever')\n",
    "    \n",
    "    \n",
    "    for k,v in datasets.items():\n",
    "        \n",
    "        print(\"we are working on:\",k)\n",
    "        corpus_scores = []\n",
    "        \n",
    "        queries = v[\"queries\"]\n",
    "        corpus = v[\"corpus\"]\n",
    "        key_ref = v[\"key_ref\"]\n",
    "        query_labels = v[\"query_labels\"]\n",
    "            \n",
    "        query_embeddings = contriever_embeddings(queries, tokenizer, model)\n",
    "        corpus_embeddings = contriever_embeddings(corpus, tokenizer, model)\n",
    "        \n",
    "        for emb1 in tqdm(query_embeddings):\n",
    "            scores = []\n",
    "            for emb2 in corpus_embeddings:\n",
    "                scores.append(1 - cosine(emb1, emb2))\n",
    "\n",
    "            corpus_scores.append(scores)\n",
    "            \n",
    "        with open(\"contriver_\"+k+\"_scores.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "            json.dump(corpus_scores,f)\n",
    "            \n",
    "        evaluation(key_ref, corpus_scores, query_labels, k)\n",
    "        print()\n",
    "        \n",
    "\n",
    "# print(\"BM25\")\n",
    "# bm25_main(source_datasets)\n",
    "\n",
    "# added this 02/26 to run with source datasets\n",
    "# abs and aspire are not in the previous script, so I removed them from model_names, as they cannot be ran\n",
    "# model_names = [\"dpr\", \"simcse-unsup\", \"simcse-sup\",\"abs\",\"aspire\"]\n",
    "model_names = [\"dpr\", \"simcse-unsup\", \"simcse-sup\"]\n",
    "for model_name in model_names:\n",
    "    print(\"============\", model_name, \"============\", \"source dataset\")\n",
    "    dpr_main(source_datasets, model_name)\n",
    "\n",
    "print(\"Contriever\")\n",
    "contriever_main(source_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83c8338",
   "metadata": {},
   "source": [
    "# Results Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "466f056c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25 agnews\n",
      "source bm25 agnews\n",
      "bm25 perspectrum\n",
      "bm25 story\n",
      "source bm25 story\n",
      "bm25 allsides\n",
      "source bm25 allsides\n",
      "bm25 exfever\n",
      "source bm25 exfever\n",
      "bm25 ambigqa\n",
      "source bm25 ambigqa\n",
      "dpr agnews\n",
      "source dpr agnews\n",
      "dpr story\n",
      "source dpr story\n",
      "dpr allsides\n",
      "source dpr allsides\n",
      "dpr exfever\n",
      "source dpr exfever\n",
      "dpr ambigqa\n",
      "source dpr ambigqa\n",
      "simcse-sup agnews\n",
      "source simcse-sup agnews\n",
      "simcse-sup story\n",
      "source simcse-sup story\n",
      "simcse-sup allsides\n",
      "source simcse-sup allsides\n",
      "simcse-sup exfever\n",
      "source simcse-sup exfever\n",
      "simcse-sup ambigqa\n",
      "source simcse-sup ambigqa\n",
      "simcse-unsup agnews\n",
      "source simcse-unsup agnews\n",
      "simcse-unsup story\n",
      "source simcse-unsup story\n",
      "simcse-unsup allsides\n",
      "source simcse-unsup allsides\n",
      "simcse-unsup exfever\n",
      "source simcse-unsup exfever\n",
      "simcse-unsup ambigqa\n",
      "source simcse-unsup ambigqa\n",
      "contriver agnews\n",
      "source contriver agnews\n",
      "contriver story\n",
      "source contriver story\n",
      "contriver allsides\n",
      "source contriver allsides\n",
      "contriver exfever\n",
      "source contriver exfever\n",
      "contriver ambigqa\n",
      "source contriver ambigqa\n"
     ]
    }
   ],
   "source": [
    "corpus_score_collection = {}\n",
    "\n",
    "for retriever in [\"bm25\",\"dpr\",\"simcse-sup\",\"simcse-unsup\",\"contriver\"]:\n",
    "    corpus_score_collection[retriever] = {}\n",
    "    for data_name in ['agnews', 'perspectrum', 'story','allsides','exfever','ambigqa']:\n",
    "        try:\n",
    "            if \"tart\" not in retriever:\n",
    "                with open(retriever+\"_\"+data_name+\"_scores.json\",\"r\") as f:\n",
    "                # had to adjust directory of the scores to match prior code\n",
    "                #with open(\"./scores/\"+retriever+\"_\"+data_name+\"_scores.json\",\"r\") as f:\n",
    "                    corpus_score_collection[retriever][data_name] = json.load(f)\n",
    "            else:\n",
    "                with open(retriever+data_name+\"_scores.json\",\"r\") as f:\n",
    "                # with open(\"./scores/\"+retriever+data_name+\"_scores.json\",\"r\") as f:\n",
    "                    corpus_score_collection[retriever][data_name] = json.load(f)                \n",
    "        except:\n",
    "            print(retriever,data_name)\n",
    "            \n",
    "        try:\n",
    "            with open(retriever+\"_source_\"+data_name+\"_scores.json\",\"r\") as f:\n",
    "            # with open(\"./scores/\"+retriever+\"_source_\"+data_name+\"_scores.json\",\"r\") as f:\n",
    "                corpus_score_collection[retriever][\"source_\"+data_name] = json.load(f)\n",
    "        except:\n",
    "            print(\"source\",retriever,data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca0d3564",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'source_agnews'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# TODO: run with source scores\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124magnews\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperspectrum\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstory\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallsides\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mambigqa\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexfever\u001b[39m\u001b[38;5;124m\"\u001b[39m]: \n\u001b[0;32m---> 61\u001b[0m     this_source_qs \u001b[38;5;241m=\u001b[39m \u001b[43msource_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msource_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mdata_name\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqueries\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     62\u001b[0m     root_mapping[data_name] \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m this_source_qs]\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i,query \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(datasets[data_name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqueries\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n",
      "\u001b[0;31mKeyError\u001b[0m: 'source_agnews'"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "def evaluation_for_writing_mrr(datasets, corpus_scores, dataset_name, thresh):\n",
    "    # evaluation of a dataset    \n",
    "    \n",
    "    key_ref = datasets[dataset_name][\"key_ref\"]\n",
    "    query_labels = datasets[dataset_name][\"query_labels\"]\n",
    "    \n",
    "    mrrs = []\n",
    "\n",
    "    for k in range(len(corpus_scores)):\n",
    "        v = key_ref[str(k)]\n",
    "        ranked_scores = (-np.array(corpus_scores[int(k)])).argsort()\n",
    "        ranked_scores = ranked_scores.tolist()\n",
    "        rr = []\n",
    "        for one_correct_doc in v:\n",
    "            rr.append(1/(ranked_scores.index(one_correct_doc)+1))\n",
    "        \n",
    "        if len(rr) == 0:\n",
    "            mrrs.append(0)\n",
    "        else:\n",
    "            mrrs.append(sum(rr)/len(rr))\n",
    "            \n",
    "    return mrrs \n",
    "\n",
    "def evaluation_for_writing_recalls(datasets, corpus_scores, dataset_name, thresh):\n",
    "    # evaluation of a dataset    \n",
    "    recall_results = []\n",
    "    \n",
    "    key_ref = datasets[dataset_name][\"key_ref\"]\n",
    "    query_labels = datasets[dataset_name][\"query_labels\"]\n",
    "\n",
    "    for k in range(len(corpus_scores)):\n",
    "        \n",
    "        v = key_ref[str(k)]\n",
    "\n",
    "        ranked_scores = (-np.array(corpus_scores[int(k)])).argsort()[:thresh]\n",
    "        indicator = 0\n",
    "        try:\n",
    "            for index in v:\n",
    "                if index in ranked_scores:\n",
    "                    indicator = 1 \n",
    "        except:\n",
    "            for index in [v]:\n",
    "                if index in ranked_scores:\n",
    "                    indicator = 1     \n",
    "                    \n",
    "        recall_results.append(indicator)\n",
    "    \n",
    "    return recall_results\n",
    "\n",
    "\n",
    "mrr_collection = {}\n",
    "\n",
    "# prepare to compute perspective-aware scores: \n",
    "# a mapping to group queries with the same root query {data_name: [[0,1,2],[3,4]]}\n",
    "root_mapping = {}\n",
    "\n",
    "# TODO: run with source scores\n",
    "for data_name in ['agnews','perspectrum', 'story','allsides','ambigqa',\"exfever\"]: \n",
    "    this_source_qs = source_datasets[\"source_\"+data_name][\"queries\"]\n",
    "    root_mapping[data_name] = [[] for x in this_source_qs]\n",
    "    \n",
    "    for i,query in enumerate(datasets[data_name][\"queries\"]):\n",
    "        sq = datasets[data_name][\"source_queries\"][i]\n",
    "        root_mapping[data_name][this_source_qs.index(sq)].append(i)\n",
    "\n",
    "# for retriever in [\"dpr\",\"simcse-sup\",\"simcse-unsup\"]:\n",
    "# FIXED uncommented line below, commented line above\n",
    "for retriever in [\"bm25\",\"dpr\",\"simcse-sup\",\"simcse-unsup\",\"contriver\"]:\n",
    "    mrr_collection[retriever] = {}\n",
    "    for data_name in ['agnews','story','perspectrum','ambigqa','allsides','exfever']: \n",
    "        per_corpus_score = corpus_score_collection[retriever][data_name]        \n",
    "        recalls = evaluation_for_writing_recalls(datasets, per_corpus_score, data_name,5)\n",
    "        \n",
    "        p_mrrs = []\n",
    "        for lst in root_mapping[data_name]:\n",
    "            temp = []\n",
    "            for x in lst:\n",
    "                if x < len(recalls):\n",
    "                    temp.append(recalls[x])\n",
    "            p_mrrs.append(temp)\n",
    "        \n",
    "        p_vars = []\n",
    "        for x in p_mrrs:\n",
    "            if len(x) > 1:\n",
    "                p_vars.append(statistics.mean(x))\n",
    "            elif len(x) == 1:\n",
    "                p_vars.append(0.0) #x[0]\n",
    "            else:\n",
    "                # equal 0\n",
    "                continue\n",
    "        \n",
    "        mrr_collection[retriever][data_name] = sum(p_vars)/len(p_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24958763",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus_score_collection.keys())  # Check available retrievers\n",
    "print(corpus_score_collection[retriever].keys())  # Check available datasets for the current retriever\n",
    "print(mrr_collection.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab1d2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 2 in the draft\n",
    "\n",
    "# for retriever in [\"bm25\",\"dpr\",\"simcse-sup\",\"simcse-unsup\",\"contriver\",\"tart\"]:\n",
    "# FIXED, uncommented line below, commented line above, as tart is not in dataset\n",
    "for retriever in [\"bm25\",\"dpr\",\"simcse-sup\",\"simcse-unsup\",\"contriver\"]:\n",
    "    # print(r_name_map[retriever],end=\" \")\n",
    "    print(retriever,end=\" \")\n",
    "    temp_str = \"\"\n",
    "    temp_score = []\n",
    "    for data_name in ['agnews','story','perspectrum','ambigqa','allsides','exfever']: \n",
    "        temp_str += \"&\"+ str(round(mrr_collection[retriever][data_name]*100,1)) + \" \"\n",
    "        temp_score.append(round(mrr_collection[retriever][data_name]*100,1))\n",
    "        \n",
    "    mean_score = sum(temp_score)/len(temp_score)\n",
    "    print(temp_str+ \"&\" + str(round(mean_score,1))+ \"\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46dea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sec 4.2 Exploring PIR\n",
    "# collection of embeddings to enable future use\n",
    "\n",
    "def extract_embeddings(datasets, model_name):\n",
    "    # corpuses,key_refs = corpus_building(datasets)\n",
    "\n",
    "    if model_name == \"dpr\":\n",
    "        ctokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "        cmodel = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "        qtokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "        qmodel = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "        \n",
    "    if model_name == \"dpr-multiset\":\n",
    "        ctokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\")\n",
    "        cmodel = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\")\n",
    "        qtokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")\n",
    "        qmodel = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")\n",
    "        \n",
    "    elif \"simcse\" in model_name:\n",
    "        model_mapping = {\n",
    "            \"simcse-unsup\":\"princeton-nlp/unsup-simcse-bert-base-uncased\",\n",
    "            \"simcse-sup\":\"princeton-nlp/sup-simcse-bert-base-uncased\",\n",
    "            \"simcse-unsup-large\":\"princeton-nlp/unsup-simcse-bert-large-uncased\",\n",
    "            \"simcse-sup-large\":\"princeton-nlp/sup-simcse-bert-large-uncased\",\n",
    "            \"simcse-unsup-rb\":\"princeton-nlp/unsup-simcse-roberta-base\",\n",
    "            \"simcse-sup-rb\":\"princeton-nlp/sup-simcse-roberta-base\",\n",
    "            \"simcse-unsup-large-rb\":\"princeton-nlp/unsup-simcse-roberta-large\",\n",
    "            \"simcse-sup-large-rb\":\"princeton-nlp/sup-simcse-roberta-large\"\n",
    "        }\n",
    "        \n",
    "        ctokenizer = AutoTokenizer.from_pretrained(model_mapping[model_name])\n",
    "        cmodel = AutoModel.from_pretrained(model_mapping[model_name])\n",
    "        qtokenizer = AutoTokenizer.from_pretrained(model_mapping[model_name])\n",
    "        qmodel = AutoModel.from_pretrained(model_mapping[model_name])\n",
    "\n",
    "        \n",
    "        \n",
    "    if model_name in [\"t5\",\"flan-t5\",\"unifiedqa\"]:\n",
    "        # encoder vs. decoder\n",
    "        model_mapping = {\n",
    "            \"t5\":\"google-t5/t5-large\",\n",
    "            \"flan-t5\": \"google/flan-t5-large\",\n",
    "            \"unifiedqa\": \"allenai/unifiedqa-v2-t5-large-1251000\"\n",
    "        }\n",
    "        \n",
    "        qtokenizer = T5Tokenizer.from_pretrained(model_mapping[model_name])\n",
    "        ctokenizer = T5Tokenizer.from_pretrained(model_mapping[model_name])\n",
    "        qmodel = T5ForConditionalGeneration.from_pretrained(model_mapping[model_name]).decoder #decoder\n",
    "        cmodel = T5ForConditionalGeneration.from_pretrained(model_mapping[model_name]).encoder #decoder\n",
    "\n",
    "    qmodel.eval() \n",
    "    cmodel.eval() \n",
    "    \n",
    "    for k,v in datasets.items():\n",
    "        \n",
    "        print(\"we are working on:\",k)\n",
    "        corpus_scores = []\n",
    "        \n",
    "        queries = v[\"queries\"]\n",
    "        source_queries = v[\"source_queries\"]\n",
    "        perspectives = v[\"perspectives\"]\n",
    "        corpus = v[\"corpus\"]\n",
    "     \n",
    "        query_embeddings = create_embeddings(qtokenizer, qmodel, queries)\n",
    "        source_query_embeddings = create_embeddings(qtokenizer, qmodel, source_queries)\n",
    "        perspectives_embeddings = create_embeddings(qtokenizer, qmodel, perspectives)\n",
    "        corpus_embeddings = create_embeddings(ctokenizer, cmodel, corpus)\n",
    "\n",
    "        names = [\"queries\",\"source_queries\",\"perspectives\",\"corpus\"]\n",
    "        embs = [query_embeddings, source_query_embeddings, perspectives_embeddings, corpus_embeddings]\n",
    "\n",
    "        for i, name in enumerate(names):\n",
    "            path = \"./embs/\"+k+\"_\"+model_name+\"_\"+name+\".json\"\n",
    "            \n",
    "            with open(path,\"w\",encoding=\"utf-8\") as f:\n",
    "                json.dump(embs[i],f)\n",
    "    \n",
    "model_lists = [\"dpr\",\"dpr-multiset\"]\n",
    "model_lists.extend([\"simcse-unsup\",\"simcse-sup\",\"simcse-unsup-large\",\"simcse-sup-large\",\"simcse-unsup-rb\",\"simcse-sup-rb\",\"simcse-unsup-large-rb\",\"simcse-sup-large-rb\"])\n",
    "\n",
    "\n",
    "for model_name in model_lists:\n",
    "    print(model_name)\n",
    "    extract_embeddings(datasets, model_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d78f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the computation below is for demo-purpose\n",
    "\n",
    "def general_pir_main(datasets, model_name=\"simcse-sup\",mode=\"vec_cast\"):\n",
    "\n",
    "    for k,v in datasets.items():\n",
    "        \n",
    "        print(\"we are working on:\",k)\n",
    "        \n",
    "        corpus_scores = []\n",
    "\n",
    "        queries = v[\"queries\"]\n",
    "        query_labels = v[\"query_labels\"]\n",
    "        corpus = v[\"corpus\"]\n",
    "        key_ref = v[\"key_ref\"]\n",
    "        perspectives = v[\"perspectives\"]\n",
    "        \n",
    "        if mode == \"bm25_ranking\":\n",
    "            tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "            bm25 = BM25Okapi(tokenized_corpus) \n",
    "        \n",
    "        embs_collection = {}\n",
    "        \n",
    "        for name in [\"queries\",\"source_queries\",\"perspectives\",\"corpus\"]:\n",
    "            with open(\"./embs/\"+k+\"_\"+model_name+\"_\"+name+\".json\",\"r\",encoding=\"utf-8\") as f:\n",
    "                embs_collection[name] = json.load(f)\n",
    "            \n",
    "        query_embeddings = embs_collection[\"queries\"]\n",
    "        source_query_embeddings = embs_collection[\"source_queries\"]\n",
    "        perspectives_embeddings = embs_collection[\"perspectives\"]\n",
    "        corpus_embeddings = embs_collection[\"corpus\"]\n",
    "        \n",
    "\n",
    "        for index in trange(len(query_embeddings)):\n",
    "            emb_q = query_embeddings[index]\n",
    "            emb_s = source_query_embeddings[index]\n",
    "            emb_p = perspectives_embeddings[index]\n",
    "            \n",
    "            scores = []\n",
    "            bm25_scores = []\n",
    "            p_scores = []\n",
    "            \n",
    "            for i, emb_c in enumerate(corpus_embeddings):\n",
    "                \n",
    "                # vector manipulation: aug denotes using q, instead of s\n",
    "                if mode == \"vec_projection_rev\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    if k == \"ambigqa\":\n",
    "                        emb_p = emb_q - emb_s\n",
    "                    weight = np.dot(emb_q, emb_p)/np.dot(emb_p, emb_p)\n",
    "                    context_score = 1 - cosine(emb_q + weight*emb_p, emb_c)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                if mode == \"vec_dual_projection_rev\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    if k == \"ambigqa\":\n",
    "                        emb_p = emb_q - emb_s\n",
    "                    p_cor = np.dot(emb_p, emb_p)\n",
    "                    weight_q = np.dot(emb_q, emb_p)/p_cor\n",
    "                    weight_c = np.dot(emb_c, emb_p)/p_cor\n",
    "                    context_score = 1 - cosine(emb_q + weight_q*emb_p, emb_c + weight_c*emb_p)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                if mode == \"vec_projection\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    if k == \"ambigqa\":\n",
    "                        emb_p = emb_q - emb_s\n",
    "                    weight = np.dot(emb_q, emb_p)/np.dot(emb_p, emb_p)\n",
    "                    context_score = 1 - cosine(emb_q - weight*emb_p, emb_c)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                if mode == \"vec_dual_projection\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    if k == \"ambigqa\":\n",
    "                        emb_p = emb_q - emb_s\n",
    "                    p_cor = np.dot(emb_p, emb_p)\n",
    "                    weight_q = np.dot(emb_q, emb_p)/p_cor\n",
    "                    weight_c = np.dot(emb_c, emb_p)/p_cor\n",
    "                    context_score = 1 - cosine(emb_q - weight_q*emb_p, emb_c- weight_c*emb_p)\n",
    "                    scores.append(context_score)\n",
    "                \n",
    "                if mode == \"vec_add\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    context_score = 1 - cosine(emb_s+emb_p, emb_c)\n",
    "                    scores.append(context_score)\n",
    "\n",
    "                if mode == \"vec_concat\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    emb_sp = np.concatenate((emb_s, emb_p), axis=None)\n",
    "                    emb_cp = np.concatenate((emb_c, emb_p), axis=None)\n",
    "                    context_score = 1 - cosine(emb_sp, emb_cp)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                if mode == \"vec_aug_add\":\n",
    "                    emb_q,emb_c,emb_p = np.array(emb_q),np.array(emb_c),np.array(emb_p)\n",
    "                    context_score = 1 - cosine(emb_q+emb_p, emb_c)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                if mode == \"vec_aug_concat\":\n",
    "                    emb_q,emb_c,emb_p = np.array(emb_q),np.array(emb_c),np.array(emb_p)\n",
    "                    emb_qp = np.concatenate((emb_q, emb_p), axis=None)\n",
    "                    emb_cp = np.concatenate((emb_c, emb_p), axis=None)\n",
    "                    context_score = 1 - cosine(emb_qp, emb_cp)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                if mode == \"vec_dual_concat\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    emb_sp = np.concatenate((emb_s, emb_p), axis=None)\n",
    "                    emb_cp = np.concatenate((emb_c, emb_p), axis=None)\n",
    "                    context_score = 1 - cosine(emb_sp, emb_cp)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                if mode == \"vec_cast_single\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    if k == \"ambigqa\":\n",
    "                        emb_p = emb_q - emb_s\n",
    "                    context_score = 1 - cosine(emb_s-emb_p, emb_c)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                if mode == \"vec_aug_cast_single\":\n",
    "                    emb_q,emb_c,emb_p = np.array(emb_q),np.array(emb_c),np.array(emb_p)\n",
    "                    if k == \"ambigqa\":\n",
    "                        emb_p = emb_q - emb_s\n",
    "                    context_score = 1 - cosine(emb_q-emb_p, emb_c)\n",
    "                    scores.append(context_score)     \n",
    "                    \n",
    "                if mode == \"vec_cast\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    if k == \"ambigqa\":\n",
    "                        emb_p = emb_q - emb_s\n",
    "                    context_score = 1 - cosine(emb_s-emb_p, emb_c-emb_p)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                if mode == \"vec_aug_cast\":\n",
    "                    emb_q,emb_c,emb_p = np.array(emb_q),np.array(emb_c),np.array(emb_p)\n",
    "                    if k == \"ambigqa\":\n",
    "                        emb_p = emb_q - emb_s\n",
    "                    context_score = 1 - cosine(emb_q-emb_p, emb_c-emb_p)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                    \n",
    "                # score manipulation\n",
    "                if mode == \"additive\":\n",
    "                    context_score = 1 - cosine(emb_s, emb_c)\n",
    "                    perspective_score = 1 - cosine(emb_p, emb_c)\n",
    "                    scores.append(context_score + perspective_score)\n",
    "                    \n",
    "                if mode == \"additive_aug\":\n",
    "                    context_score = 1 - cosine(emb_q, emb_c)\n",
    "                    perspective_score = 1 - cosine(emb_p, emb_c)\n",
    "                    scores.append(context_score + perspective_score)\n",
    "                    \n",
    "                if mode == \"additive_tripple\":\n",
    "                    source_score = 1 - cosine(emb_s, emb_c)\n",
    "                    context_score = 1 - cosine(emb_q, emb_c)\n",
    "                    perspective_score = 1 - cosine(emb_p, emb_c)\n",
    "                    scores.append(source_score + context_score + perspective_score)\n",
    "                    \n",
    "                \n",
    "                # re-ranking\n",
    "                if mode == \"re-ranking\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    q_score = 1 - cosine(emb_s, emb_c)\n",
    "                    context_score = 1 - cosine(emb_p, emb_c)\n",
    "                    scores.append(q_score)\n",
    "                    p_scores.append(context_score)\n",
    "\n",
    "                if mode == \"re-ranking_aug\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    q_score = 1 - cosine(emb_q, emb_c)\n",
    "                    context_score = 1 - cosine(emb_p, emb_c)\n",
    "                    scores.append(q_score)\n",
    "                    p_scores.append(context_score)\n",
    "                \n",
    "                if mode == \"bm25_ranking\":\n",
    "                    query = queries[index]\n",
    "                    context = corpus[index]\n",
    "                    kws = perspectives[index].split(\" \")\n",
    "\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    context_score = 1 - cosine(emb_s-emb_p, emb_c-emb_p)\n",
    "                    scores.append(context_score)\n",
    "\n",
    "                    tokenized_query = query.split(\" \")\n",
    "                    doc_scores = bm25.get_scores(tokenized_query)\n",
    "                    bm25_scores.append(doc_scores[i])\n",
    "            \n",
    "            if mode == \"re-ranking\" or mode == \"re-ranking_aug\":\n",
    "                thresh = float(np.percentile(np.array(scores), 80))\n",
    "                temp_scores = []\n",
    "                for i, score in enumerate(scores):\n",
    "                    if score >= thresh:\n",
    "                        temp_score = score + p_scores[i]\n",
    "                    else:\n",
    "                        temp_score = -100\n",
    "                scores = temp_scores\n",
    "            \n",
    "            if mode == \"bm25_ranking\":\n",
    "                sm = statistics.mean(scores)\n",
    "                bm25m = statistics.mean(bm25_scores)\n",
    "                temp_scores = []\n",
    "                for i, score in enumerate(scores):\n",
    "                    temp_scores = score-sm+bm25_scores[i]-bm25m\n",
    "                scores = temp_scores\n",
    "                    \n",
    "            corpus_scores.append(scores)\n",
    "            \n",
    "        with open(\"./pir_scores/pir_\"+k+\"_\"+mode+\"_scores.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "            json.dump(corpus_scores,f)\n",
    "            \n",
    "        evaluation(key_ref, corpus_scores, query_labels, k)\n",
    "        print()\n",
    "\n",
    "all_pir_mode = []\n",
    "\n",
    "# all_pir_mode.extend([\"vec_add\",\"vec_concat\",\"vec_aug_add\",\"vec_aug_concat\",\"vec_dual_concat\",\"vec_cast\",\"vec_aug_cast\"])\n",
    "# all_pir_mode.extend([\"additive\",\"additive_aug\",\"additive_tripple\"])\n",
    "# all_pir_mode.extend([\"vec_cast_single\",\"vec_aug_cast_single\", \"re-ranking\", \"re-ranking_aug\",\"bm25_ranking\"])\n",
    "\n",
    "# all_pir_mode.extend([\"vec_projection\",\"vec_dual_projection\",\"vec_cast\",\"vec_aug_cast\"])\n",
    "\n",
    "# all_pir_mode.extend([\"vec_projection_rev\",\"vec_dual_projection_rev\"])\n",
    "# for mode in all_pir_mode:\n",
    "#     print(\"-------------------\",mode,\"--------------------------\")\n",
    "#     general_pir_main(datasets,model_name=\"simcse-sup\",mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddb2112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the PIR scores computed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
