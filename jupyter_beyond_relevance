{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e4d5330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "\n",
    "import openai\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy import stats\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer,T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer,DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "\n",
    "import logging\n",
    "import transformers\n",
    "transformers.tokenization_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.configuration_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.modeling_utils.logger.setLevel(logging.ERROR)\n",
    "\n",
    "import random\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00dc75af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perspectrum\n",
      "Query size and length 100 15.01\n",
      "Corpus size and length 500 11.026\n",
      "agnews\n",
      "Query size and length 100 84.39\n",
      "Corpus size and length 500 162.01\n",
      "story\n",
      "Query size and length 100 24.58\n",
      "Corpus size and length 500 15.916\n",
      "ambigqa\n",
      "Query size and length 100 12.32\n",
      "Corpus size and length 500 28.858\n",
      "allsides\n",
      "Query size and length 100 12.21\n",
      "Corpus size and length 500 1075.936\n",
      "exfever\n",
      "Query size and length 100 49.37\n",
      "Corpus size and length 500 28.378\n"
     ]
    }
   ],
   "source": [
    "path = \"./demo_pir_dataset.json\"\n",
    "    \n",
    "with open(path,\"r\",encoding=\"utf-8\") as f:\n",
    "    datasets = json.load(f)\n",
    "\n",
    "for k,v in datasets.items():\n",
    "    print(k)\n",
    "    q_len = [len(x.split()) for x in v[\"queries\"]]\n",
    "    c_len = [len(x.split()) for x in v[\"corpus\"]]\n",
    "\n",
    "    print(\"Query size and length\",len(v[\"queries\"]),sum(q_len)/len(q_len))\n",
    "    print(\"Corpus size and length\",len(v[\"corpus\"]),sum(c_len)/len(c_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f57abd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_perspectrum\n",
      "16\n",
      "Query size and length 16 7.9375\n",
      "Corpus size and length 500 11.026\n",
      "source_agnews\n",
      "50\n",
      "Query size and length 50 69.86\n",
      "Corpus size and length 500 162.01\n",
      "source_story\n",
      "50\n",
      "Query size and length 50 13.08\n",
      "Corpus size and length 500 15.916\n",
      "source_ambigqa\n",
      "26\n",
      "Query size and length 26 9.23076923076923\n",
      "Corpus size and length 500 28.858\n",
      "source_allsides\n",
      "17\n",
      "Query size and length 17 1.588235294117647\n",
      "Corpus size and length 500 1075.936\n",
      "source_exfever\n",
      "34\n",
      "Query size and length 34 40.529411764705884\n",
      "Corpus size and length 500 28.378\n"
     ]
    }
   ],
   "source": [
    "# create a root query only dataset\n",
    "source_datasets = {}\n",
    "\n",
    "for data_name, dataset in datasets.items():\n",
    "    # {\"queries\":[],\"source_queries\":[],\"perspectives\":[],\"corpus\":[],\"key_ref\":{},\"query_labels\":[]}\n",
    "    source_datasets[\"source_\"+data_name] = {\"corpus\":dataset[\"corpus\"],\"queries\":[],\"source_queries\":[],\"perspectives\":[],\"key_ref\":{},\"query_labels\":[]}\n",
    "    \n",
    "    reverse_source_query_dic = {}\n",
    "    \n",
    "    for i, query in enumerate(dataset[\"source_queries\"]):\n",
    "        if query not in list(reverse_source_query_dic.keys()):\n",
    "            query_id = str(len(source_datasets[\"source_\"+data_name][\"queries\"]))\n",
    "            reverse_source_query_dic[query] = query_id\n",
    "            source_datasets[\"source_\"+data_name][\"queries\"].append(query)\n",
    "            source_datasets[\"source_\"+data_name][\"source_queries\"].append(query)\n",
    "            source_datasets[\"source_\"+data_name][\"perspectives\"].append(\"none\")\n",
    "            source_datasets[\"source_\"+data_name][\"query_labels\"].append(\"none\")\n",
    "            source_datasets[\"source_\"+data_name][\"key_ref\"][query_id] = dataset[\"key_ref\"][str(i)]\n",
    "        else:\n",
    "            # this source query already exists\n",
    "            source_datasets[\"source_\"+data_name][\"key_ref\"][str(reverse_source_query_dic[query])].extend(dataset[\"key_ref\"][str(i)])\n",
    "\n",
    "        \n",
    "for k,v in source_datasets.items():\n",
    "    print(k)\n",
    "    print(len(v[\"key_ref\"].keys()))\n",
    "    \n",
    "    q_len = [len(x.split()) for x in v[\"queries\"]]\n",
    "    c_len = [len(x.split()) for x in v[\"corpus\"]]\n",
    "\n",
    "    print(\"Query size and length\",len(v[\"queries\"]),sum(q_len)/len(q_len))\n",
    "    print(\"Corpus size and length\",len(v[\"corpus\"]),sum(c_len)/len(c_len))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199f9893",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1885912216.py, line 315)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[28], line 315\u001b[0;36m\u001b[0m\n\u001b[0;31m    running the processing on source datasets by changing the names of the datasets\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def evaluation(key_ref, corpus_scores, query_labels, dataset_name):\n",
    "    # evaluation of a dataset    \n",
    "    recall_threshold = [1,5,10]\n",
    "    recall_results = [0 for thresh in recall_threshold]\n",
    "    \n",
    "    if \"source\" in dataset_name:\n",
    "        parts = [\"none\"]\n",
    "    else:\n",
    "        if dataset_name == \"perspectrum\":\n",
    "            parts = [\"support\",\"undermine\",\"general\"]\n",
    "        elif dataset_name == \"agnews\":\n",
    "            parts = [\"subtopic\", \"location\"]\n",
    "        elif dataset_name == \"story\":\n",
    "            parts = [\"analogy\", \"entity\"]\n",
    "        elif dataset_name == \"ambigqa\":\n",
    "            parts = [\"perspective\"]\n",
    "        elif dataset_name == \"allsides\":\n",
    "            parts = [\"left\",\"right\",\"center\"]\n",
    "        elif dataset_name == \"exfever\":\n",
    "            parts = [\"SUPPORT\",\"REFUTE\",\"NOT ENOUGH INFO\"]\n",
    "    \n",
    "    parts_size = [0 for x in parts]\n",
    "        \n",
    "    for lb in query_labels:\n",
    "        parts_size[parts.index(lb)] += 1\n",
    "            \n",
    "    partial_recall_results = []\n",
    "    for i in range(len(parts)):\n",
    "        partial_recall_results.append([0 for thresh in recall_threshold])\n",
    "\n",
    "    \n",
    "    for k,v in key_ref.items():\n",
    "        for j, thresh in enumerate(recall_threshold):\n",
    "            # important: find one is ok, this can be modified\n",
    "            ranked_scores = (-np.array(corpus_scores[int(k)])).argsort()[:thresh]\n",
    "            \n",
    "            \n",
    "            indicator = 0\n",
    "            try:\n",
    "                for index in v:\n",
    "                    if index in ranked_scores:\n",
    "                        indicator = 1 \n",
    "            except:\n",
    "                for index in [v]:\n",
    "                    if index in ranked_scores:\n",
    "                        indicator = 1                \n",
    "            recall_results[j] += indicator\n",
    "            partial_recall_results[parts.index(query_labels[int(k)])][j] += indicator\n",
    "    \n",
    "    final_results = [result/len(key_ref.items()) for result in recall_results]\n",
    "        \n",
    "    print(\"overall\")\n",
    "    for i, thresh in enumerate(recall_threshold):\n",
    "        print(\"Recall@\"+str(thresh)+\":\",final_results[i])\n",
    "        \n",
    "    macro_threshs = [[] for x in recall_threshold]\n",
    "    \n",
    "    for t, recall_results in enumerate(partial_recall_results):\n",
    "        print(parts[t])\n",
    "        final_results = [result/parts_size[t] for result in recall_results]\n",
    "        \n",
    "        for i, thresh in enumerate(recall_threshold):\n",
    "            print(\"Recall@\"+str(thresh)+\":\",final_results[i])\n",
    "            macro_threshs[i].append(final_results[i])\n",
    "                \n",
    "    print(\"macro_average\")\n",
    "    for i, thresh in enumerate(recall_threshold):\n",
    "        print(\"Recall@\"+str(thresh)+\":\",sum(macro_threshs[i])/len(macro_threshs[i]))\n",
    "                \n",
    "                    \n",
    "\n",
    "# BM25 and BERTScore\n",
    "from rank_bm25 import BM25Okapi\n",
    "from evaluate import load\n",
    "\n",
    "\n",
    "import logging\n",
    "import transformers\n",
    "transformers.tokenization_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.configuration_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.modeling_utils.logger.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "def bm25_main(datasets):\n",
    "    # corpuses,key_refs = corpus_building(datasets)\n",
    "\n",
    "    for k,v in datasets.items():\n",
    "        print(\"we are working on:\",k)\n",
    "        \n",
    "        queries = v[\"queries\"]\n",
    "        corpus = v[\"corpus\"]\n",
    "        key_ref = v[\"key_ref\"]\n",
    "        query_labels = v[\"query_labels\"]\n",
    "        \n",
    "        tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "        bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "        corpus_scores = []\n",
    "\n",
    "        for query in tqdm(queries):\n",
    "            # query = item[\"query\"]\n",
    "            tokenized_query = query.split(\" \")\n",
    "            doc_scores = bm25.get_scores(tokenized_query)\n",
    "            corpus_scores.append(doc_scores)\n",
    "        \n",
    "        with open(\"bm25_\"+k+\"_scores.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "            json.dump([x.tolist() for x in corpus_scores],f)\n",
    "        \n",
    "        evaluation(key_ref, corpus_scores, query_labels, k)\n",
    "        print()\n",
    "        \n",
    "        \n",
    "def extract_layer_cls(embeddings,layer):\n",
    "    rep = []\n",
    "    this_layer_embeddings = embeddings[layer]\n",
    "    for emb in this_layer_embeddings:\n",
    "        rep.append(emb[0])\n",
    "\n",
    "\n",
    "    return rep\n",
    "\n",
    "        \n",
    "def create_embeddings(tokenizer, model, texts):\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        \n",
    "    # create tokenized inputs\n",
    "    batch_size = 17 #29\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # naive batching\n",
    "    if len(texts) < batch_size:\n",
    "        inputs = tokenizer(texts,max_length=80, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        inputs = inputs.to(device)\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output\n",
    "            embeddings = []\n",
    "            for embedding in batch_embeddings:\n",
    "                embeddings.append(embedding.detach().cpu().tolist())\n",
    "            del batch_embeddings\n",
    "            torch.cuda.empty_cache()\n",
    "    else:\n",
    "        embeddings = []\n",
    "        num_batch = len(texts)//batch_size\n",
    "\n",
    "        for i in trange(num_batch+1):\n",
    "            batch_start = i*batch_size\n",
    "            batch_end = min(len(texts), (i+1)*batch_size)\n",
    "            batch_texts = texts[batch_start:batch_end]\n",
    "\n",
    "            inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    batch_embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output\n",
    "                    embeddings.extend(batch_embeddings.detach().cpu().tolist())\n",
    "\n",
    "                    # save cuda memory\n",
    "                    del batch_embeddings\n",
    "                    del inputs\n",
    "                    torch.cuda.empty_cache()\n",
    "                except:\n",
    "                    message = \"broken embeddings\"\n",
    "\n",
    "    # 25 * num_example * seq_len * 768 -> num_example * 768\n",
    "    return embeddings\n",
    "        \n",
    "    \n",
    "def dpr_main(datasets, model_name):\n",
    "    # corpuses,key_refs = corpus_building(datasets)\n",
    "\n",
    "    if model_name == \"dpr\":\n",
    "        ctokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "        cmodel = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "        qtokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "        qmodel = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "        \n",
    "    elif model_name in [\"simcse-unsup\",\"simcse-sup\"]:\n",
    "        model_mapping = {\n",
    "            \"simcse-unsup\":\"princeton-nlp/unsup-simcse-bert-base-uncased\",\n",
    "            \"simcse-sup\":\"princeton-nlp/sup-simcse-bert-base-uncased\",\n",
    "        }\n",
    "        \n",
    "        ctokenizer = AutoTokenizer.from_pretrained(model_mapping[model_name])\n",
    "        cmodel = AutoModel.from_pretrained(model_mapping[model_name])\n",
    "        qtokenizer = AutoTokenizer.from_pretrained(model_mapping[model_name])\n",
    "        qmodel = AutoModel.from_pretrained(model_mapping[model_name])\n",
    "\n",
    "            \n",
    "    qmodel.eval() \n",
    "    cmodel.eval() \n",
    "    \n",
    "    for k,v in datasets.items():\n",
    "        \n",
    "        print(\"we are working on:\",k)\n",
    "        corpus_scores = []\n",
    "        \n",
    "        queries = v[\"queries\"]\n",
    "        corpus = v[\"corpus\"]\n",
    "        key_ref = v[\"key_ref\"]\n",
    "        query_labels = v[\"query_labels\"]\n",
    "            \n",
    "        if model_name in [\"t5\",\"flan-t5\",\"unifiedqa\"]:\n",
    "            query_embeddings = create_T5_embeddings(qtokenizer, qmodel, queries,0)\n",
    "            corpus_embeddings = create_T5_embeddings(ctokenizer, cmodel, corpus, 0)  \n",
    "        else:\n",
    "            query_embeddings = create_embeddings(qtokenizer, qmodel, queries)\n",
    "            corpus_embeddings = create_embeddings(ctokenizer, cmodel, corpus)\n",
    "        \n",
    "        for emb1 in tqdm(query_embeddings):\n",
    "            scores = []\n",
    "            for emb2 in corpus_embeddings:\n",
    "                scores.append(1 - cosine(emb1, emb2))\n",
    "\n",
    "            corpus_scores.append(scores)\n",
    "            \n",
    "        with open(model_name+\"_\"+k+\"_scores.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "            json.dump(corpus_scores,f)\n",
    "            \n",
    "        evaluation(key_ref, corpus_scores, query_labels, k)\n",
    "        print()\n",
    "        \n",
    "        \n",
    "def contriever_main(datasets):\n",
    "    # corpuses,key_refs = corpus_building(datasets)\n",
    "    \n",
    "    def mean_pooling(token_embeddings, mask):\n",
    "        token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\n",
    "        sentence_embeddings = token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]\n",
    "        \n",
    "        return sentence_embeddings\n",
    "    \n",
    "    def contriever_embeddings(texts, tokenizer, model):\n",
    "        # device = torch.device('cuda')\n",
    "        device = torch.device('cpu')\n",
    "        # create tokenized inputs\n",
    "        batch_size = 29\n",
    "\n",
    "        model.to(device)\n",
    "        embeddings = []\n",
    "        # naive batching\n",
    "        if len(texts) < batch_size:\n",
    "            inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "            outputs = model(**inputs)    \n",
    "            batch_embeddings = mean_pooling(outputs[0], inputs['attention_mask'])\n",
    "            for embedding in batch_embeddings:\n",
    "                embeddings.append(embedding.detach().cpu().tolist())\n",
    "                \n",
    "            del batch_embeddings\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            num_batch = len(texts)//batch_size\n",
    "\n",
    "            for i in trange(num_batch+1):\n",
    "                batch_start = i*batch_size\n",
    "                batch_end = min(len(texts), (i+1)*batch_size)\n",
    "                batch_texts = texts[batch_start:batch_end]\n",
    "\n",
    "                inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    try:\n",
    "                        batch_embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output\n",
    "                        embeddings.extend(batch_embeddings.detach().cpu().tolist())\n",
    "                        del batch_embeddings\n",
    "                        del inputs\n",
    "                        torch.cuda.empty_cache()\n",
    "                    except:\n",
    "                        message = \"broken embeddings\"   \n",
    "                        \n",
    "        return embeddings\n",
    "    \n",
    "    \n",
    "#     tokenizer = AutoTokenizer.from_pretrained('facebook/contriever-msmarco')\n",
    "#     model = AutoModel.from_pretrained('facebook/contriever-msmarco')\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained('facebook/contriever')\n",
    "    model = AutoModel.from_pretrained('facebook/contriever')\n",
    "    \n",
    "    \n",
    "    for k,v in datasets.items():\n",
    "        \n",
    "        print(\"we are working on:\",k)\n",
    "        corpus_scores = []\n",
    "        \n",
    "        queries = v[\"queries\"]\n",
    "        corpus = v[\"corpus\"]\n",
    "        key_ref = v[\"key_ref\"]\n",
    "        query_labels = v[\"query_labels\"]\n",
    "            \n",
    "        query_embeddings = contriever_embeddings(queries, tokenizer, model)\n",
    "        corpus_embeddings = contriever_embeddings(corpus, tokenizer, model)\n",
    "        \n",
    "        for emb1 in tqdm(query_embeddings):\n",
    "            scores = []\n",
    "            for emb2 in corpus_embeddings:\n",
    "                scores.append(1 - cosine(emb1, emb2))\n",
    "\n",
    "            corpus_scores.append(scores)\n",
    "            \n",
    "        with open(\"contriver_\"+k+\"_scores.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "            json.dump(corpus_scores,f)\n",
    "            \n",
    "        evaluation(key_ref, corpus_scores, query_labels, k)\n",
    "        print()\n",
    "        \n",
    "\n",
    "\n",
    "# running the processing on source datasets by changing the names of the datasets\n",
    "        \n",
    "print(\"BM25\")\n",
    "bm25_main(datasets)\n",
    "\n",
    "model_names = [\"dpr\", \"simcse-unsup\", \"simcse-sup\",\"abs\",\"aspire\"]\n",
    "for model_name in model_names:\n",
    "    print(\"============\", model_name, \"============\")\n",
    "    dpr_main(datasets, model_name) \n",
    "\n",
    "print(\"Contriever\")\n",
    "contriever_main(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83c8338",
   "metadata": {},
   "source": [
    "# Results Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "466f056c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25 agnews\n",
      "source bm25 agnews\n",
      "bm25 perspectrum\n",
      "source bm25 perspectrum\n",
      "bm25 story\n",
      "source bm25 story\n",
      "bm25 allsides\n",
      "source bm25 allsides\n",
      "bm25 exfever\n",
      "source bm25 exfever\n",
      "bm25 ambigqa\n",
      "source bm25 ambigqa\n",
      "dpr agnews\n",
      "source dpr agnews\n",
      "dpr perspectrum\n",
      "source dpr perspectrum\n",
      "dpr story\n",
      "source dpr story\n",
      "dpr allsides\n",
      "source dpr allsides\n",
      "dpr exfever\n",
      "source dpr exfever\n",
      "dpr ambigqa\n",
      "source dpr ambigqa\n",
      "simcse-sup agnews\n",
      "source simcse-sup agnews\n",
      "simcse-sup perspectrum\n",
      "source simcse-sup perspectrum\n",
      "simcse-sup story\n",
      "source simcse-sup story\n",
      "simcse-sup allsides\n",
      "source simcse-sup allsides\n",
      "simcse-sup exfever\n",
      "source simcse-sup exfever\n",
      "simcse-sup ambigqa\n",
      "source simcse-sup ambigqa\n",
      "simcse-unsup agnews\n",
      "source simcse-unsup agnews\n",
      "simcse-unsup perspectrum\n",
      "source simcse-unsup perspectrum\n",
      "simcse-unsup story\n",
      "source simcse-unsup story\n",
      "simcse-unsup allsides\n",
      "source simcse-unsup allsides\n",
      "simcse-unsup exfever\n",
      "source simcse-unsup exfever\n",
      "simcse-unsup ambigqa\n",
      "source simcse-unsup ambigqa\n",
      "contriver agnews\n",
      "source contriver agnews\n",
      "contriver perspectrum\n",
      "source contriver perspectrum\n",
      "contriver story\n",
      "source contriver story\n",
      "contriver allsides\n",
      "source contriver allsides\n",
      "contriver exfever\n",
      "source contriver exfever\n",
      "contriver ambigqa\n",
      "source contriver ambigqa\n"
     ]
    }
   ],
   "source": [
    "corpus_score_collection = {}\n",
    "\n",
    "for retriever in [\"bm25\",\"dpr\",\"simcse-sup\",\"simcse-unsup\",\"contriver\"]:\n",
    "    corpus_score_collection[retriever] = {}\n",
    "    for data_name in ['agnews', 'perspectrum', 'story','allsides','exfever','ambigqa']:\n",
    "        try:\n",
    "            if \"tart\" not in retriever:\n",
    "                with open(\"./scores/\"+retriever+\"_\"+data_name+\"_scores.json\",\"r\") as f:\n",
    "                    corpus_score_collection[retriever][data_name] = json.load(f)\n",
    "            else:\n",
    "                with open(\"./scores/\"+retriever+data_name+\"_scores.json\",\"r\") as f:\n",
    "                    corpus_score_collection[retriever][data_name] = json.load(f)                \n",
    "        except:\n",
    "            print(retriever,data_name)\n",
    "            \n",
    "        try:\n",
    "            with open(\"./scores/\"+retriever+\"_source_\"+data_name+\"_scores.json\",\"r\") as f:\n",
    "                corpus_score_collection[retriever][\"source_\"+data_name] = json.load(f)\n",
    "        except:\n",
    "            print(\"source\",retriever,data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca0d3564",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'agnews'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m mrr_collection[retriever] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124magnews\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstory\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperspectrum\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mambigqa\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallsides\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexfever\u001b[39m\u001b[38;5;124m'\u001b[39m]: \n\u001b[0;32m---> 70\u001b[0m     per_corpus_score \u001b[38;5;241m=\u001b[39m \u001b[43mcorpus_score_collection\u001b[49m\u001b[43m[\u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata_name\u001b[49m\u001b[43m]\u001b[49m        \n\u001b[1;32m     71\u001b[0m     recalls \u001b[38;5;241m=\u001b[39m evaluation_for_writing_recalls(datasets, per_corpus_score, data_name,\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     73\u001b[0m     p_mrrs \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyError\u001b[0m: 'agnews'"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "def evaluation_for_writing_mrr(datasets, corpus_scores, dataset_name, thresh):\n",
    "    # evaluation of a dataset    \n",
    "    \n",
    "    key_ref = datasets[dataset_name][\"key_ref\"]\n",
    "    query_labels = datasets[dataset_name][\"query_labels\"]\n",
    "    \n",
    "    mrrs = []\n",
    "\n",
    "    for k in range(len(corpus_scores)):\n",
    "        v = key_ref[str(k)]\n",
    "        ranked_scores = (-np.array(corpus_scores[int(k)])).argsort()\n",
    "        ranked_scores = ranked_scores.tolist()\n",
    "        rr = []\n",
    "        for one_correct_doc in v:\n",
    "            rr.append(1/(ranked_scores.index(one_correct_doc)+1))\n",
    "        \n",
    "        if len(rr) == 0:\n",
    "            mrrs.append(0)\n",
    "        else:\n",
    "            mrrs.append(sum(rr)/len(rr))\n",
    "            \n",
    "    return mrrs \n",
    "\n",
    "def evaluation_for_writing_recalls(datasets, corpus_scores, dataset_name, thresh):\n",
    "    # evaluation of a dataset    \n",
    "    recall_results = []\n",
    "    \n",
    "    key_ref = datasets[dataset_name][\"key_ref\"]\n",
    "    query_labels = datasets[dataset_name][\"query_labels\"]\n",
    "\n",
    "    for k in range(len(corpus_scores)):\n",
    "        \n",
    "        v = key_ref[str(k)]\n",
    "\n",
    "        ranked_scores = (-np.array(corpus_scores[int(k)])).argsort()[:thresh]\n",
    "        indicator = 0\n",
    "        try:\n",
    "            for index in v:\n",
    "                if index in ranked_scores:\n",
    "                    indicator = 1 \n",
    "        except:\n",
    "            for index in [v]:\n",
    "                if index in ranked_scores:\n",
    "                    indicator = 1     \n",
    "                    \n",
    "        recall_results.append(indicator)\n",
    "    \n",
    "    return recall_results\n",
    "\n",
    "\n",
    "mrr_collection = {}\n",
    "\n",
    "# prepare to compute perspective-aware scores: \n",
    "# a mapping to group queries with the same root query {data_name: [[0,1,2],[3,4]]}\n",
    "root_mapping = {}\n",
    "\n",
    "for data_name in ['agnews','perspectrum', 'story','allsides','ambigqa',\"exfever\"]: \n",
    "    this_source_qs = source_datasets[\"source_\"+data_name][\"queries\"]\n",
    "    root_mapping[data_name] = [[] for x in this_source_qs]\n",
    "    \n",
    "    for i,query in enumerate(datasets[data_name][\"queries\"]):\n",
    "        sq = datasets[data_name][\"source_queries\"][i]\n",
    "        root_mapping[data_name][this_source_qs.index(sq)].append(i)\n",
    "\n",
    "for retriever in [\"bm25\",\"dpr\",\"simcse-sup\",\"simcse-unsup\",\"contriver\"]:\n",
    "    mrr_collection[retriever] = {}\n",
    "    for data_name in ['agnews','story','perspectrum','ambigqa','allsides','exfever']: \n",
    "        per_corpus_score = corpus_score_collection[retriever][data_name]        \n",
    "        recalls = evaluation_for_writing_recalls(datasets, per_corpus_score, data_name,5)\n",
    "        \n",
    "        p_mrrs = []\n",
    "        for lst in root_mapping[data_name]:\n",
    "            temp = []\n",
    "            for x in lst:\n",
    "                if x < len(recalls):\n",
    "                    temp.append(recalls[x])\n",
    "            p_mrrs.append(temp)\n",
    "        \n",
    "        p_vars = []\n",
    "        for x in p_mrrs:\n",
    "            if len(x) > 1:\n",
    "                p_vars.append(statistics.mean(x))\n",
    "            elif len(x) == 1:\n",
    "                p_vars.append(0.0) #x[0]\n",
    "            else:\n",
    "                # equal 0\n",
    "                continue\n",
    "        \n",
    "        mrr_collection[retriever][data_name] = sum(p_vars)/len(p_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24958763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['bm25', 'dpr', 'simcse-sup', 'simcse-unsup', 'contriver'])\n",
      "dict_keys([])\n"
     ]
    }
   ],
   "source": [
    "print(corpus_score_collection.keys())  # Check available retrievers\n",
    "print(corpus_score_collection[retriever].keys())  # Check available datasets for the current retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ab1d2d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'r_name_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Table 2 in the draft\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m retriever \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbm25\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimcse-sup\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimcse-unsup\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontriver\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtart\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mr_name_map\u001b[49m[retriever],end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     temp_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m     temp_score \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'r_name_map' is not defined"
     ]
    }
   ],
   "source": [
    "# Table 2 in the draft\n",
    "\n",
    "for retriever in [\"bm25\",\"dpr\",\"simcse-sup\",\"simcse-unsup\",\"contriver\",\"tart\"]:\n",
    "    print(r_name_map[retriever],end=\" \")\n",
    "    temp_str = \"\"\n",
    "    temp_score = []\n",
    "    for data_name in ['agnews','story','perspectrum','ambigqa','allsides','exfever']: \n",
    "        temp_str += \"&\"+ str(round(mrr_collection[retriever][data_name]*100,1)) + \" \"\n",
    "        temp_score.append(round(mrr_collection[retriever][data_name]*100,1))\n",
    "        \n",
    "    mean_score = sum(temp_score)/len(temp_score)\n",
    "    print(temp_str+ \"&\" + str(round(mean_score,1))+ \"\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c46dea02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpr\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e6760c1ade4fc18cf3964b242643f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e60153b72b64972a73d974db43afd14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a659f708036d47f99917aeb3b277dfc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1963a650bbba4b8bb0718cc1301a54f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/492 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139db577b07147e2b43926a9ed54b71e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266494988ff34b939553de2ff46aa666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b23444277f4c089862b0d47ed7ef46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7014b03e506b4a678632867c5fc9fa08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a129d66fd949c2b6a7b14025d2ad15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e794c4128640480fb88c7760c9e6e65a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/493 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8a368293c4414ea9bec3bd82d85c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are working on: perspectrum\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1374b31058453289fac5b0cd35fcb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef796ce5a58c4fd1bf8d3bb1e23ceb77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5cc43b773749c08194dbfaa892af30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98159d43a94b468ba918d6dec912ef9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45cb72e87254e3781a85d64fb34b4e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './embs/perspectrum_dpr_queries.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m model_lists:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model_name)\n\u001b[0;32m---> 84\u001b[0m     \u001b[43mextract_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 75\u001b[0m, in \u001b[0;36mextract_embeddings\u001b[0;34m(datasets, model_name)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(names):\n\u001b[1;32m     73\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./embs/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mk\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mmodel_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mname\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     76\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(embs[i],f)\n",
      "File \u001b[0;32m~/.conda/envs/pir/lib/python3.13/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './embs/perspectrum_dpr_queries.json'"
     ]
    }
   ],
   "source": [
    "# Sec 4.2 Exploring PIR\n",
    "# collection of embeddings to enable future use\n",
    "\n",
    "def extract_embeddings(datasets, model_name):\n",
    "    # corpuses,key_refs = corpus_building(datasets)\n",
    "\n",
    "    if model_name == \"dpr\":\n",
    "        ctokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "        cmodel = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "        qtokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "        qmodel = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "        \n",
    "    if model_name == \"dpr-multiset\":\n",
    "        ctokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\")\n",
    "        cmodel = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\")\n",
    "        qtokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")\n",
    "        qmodel = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")\n",
    "        \n",
    "    elif \"simcse\" in model_name:\n",
    "        model_mapping = {\n",
    "            \"simcse-unsup\":\"princeton-nlp/unsup-simcse-bert-base-uncased\",\n",
    "            \"simcse-sup\":\"princeton-nlp/sup-simcse-bert-base-uncased\",\n",
    "            \"simcse-unsup-large\":\"princeton-nlp/unsup-simcse-bert-large-uncased\",\n",
    "            \"simcse-sup-large\":\"princeton-nlp/sup-simcse-bert-large-uncased\",\n",
    "            \"simcse-unsup-rb\":\"princeton-nlp/unsup-simcse-roberta-base\",\n",
    "            \"simcse-sup-rb\":\"princeton-nlp/sup-simcse-roberta-base\",\n",
    "            \"simcse-unsup-large-rb\":\"princeton-nlp/unsup-simcse-roberta-large\",\n",
    "            \"simcse-sup-large-rb\":\"princeton-nlp/sup-simcse-roberta-large\"\n",
    "        }\n",
    "        \n",
    "        ctokenizer = AutoTokenizer.from_pretrained(model_mapping[model_name])\n",
    "        cmodel = AutoModel.from_pretrained(model_mapping[model_name])\n",
    "        qtokenizer = AutoTokenizer.from_pretrained(model_mapping[model_name])\n",
    "        qmodel = AutoModel.from_pretrained(model_mapping[model_name])\n",
    "\n",
    "        \n",
    "        \n",
    "    if model_name in [\"t5\",\"flan-t5\",\"unifiedqa\"]:\n",
    "        # encoder vs. decoder\n",
    "        model_mapping = {\n",
    "            \"t5\":\"google-t5/t5-large\",\n",
    "            \"flan-t5\": \"google/flan-t5-large\",\n",
    "            \"unifiedqa\": \"allenai/unifiedqa-v2-t5-large-1251000\"\n",
    "        }\n",
    "        \n",
    "        qtokenizer = T5Tokenizer.from_pretrained(model_mapping[model_name])\n",
    "        ctokenizer = T5Tokenizer.from_pretrained(model_mapping[model_name])\n",
    "        qmodel = T5ForConditionalGeneration.from_pretrained(model_mapping[model_name]).decoder #decoder\n",
    "        cmodel = T5ForConditionalGeneration.from_pretrained(model_mapping[model_name]).encoder #decoder\n",
    "\n",
    "    qmodel.eval() \n",
    "    cmodel.eval() \n",
    "    \n",
    "    for k,v in datasets.items():\n",
    "        \n",
    "        print(\"we are working on:\",k)\n",
    "        corpus_scores = []\n",
    "        \n",
    "        queries = v[\"queries\"]\n",
    "        source_queries = v[\"source_queries\"]\n",
    "        perspectives = v[\"perspectives\"]\n",
    "        corpus = v[\"corpus\"]\n",
    "     \n",
    "        query_embeddings = create_embeddings(qtokenizer, qmodel, queries)\n",
    "        source_query_embeddings = create_embeddings(qtokenizer, qmodel, source_queries)\n",
    "        perspectives_embeddings = create_embeddings(qtokenizer, qmodel, perspectives)\n",
    "        corpus_embeddings = create_embeddings(ctokenizer, cmodel, corpus)\n",
    "\n",
    "        names = [\"queries\",\"source_queries\",\"perspectives\",\"corpus\"]\n",
    "        embs = [query_embeddings, source_query_embeddings, perspectives_embeddings, corpus_embeddings]\n",
    "\n",
    "        for i, name in enumerate(names):\n",
    "            path = \"./embs/\"+k+\"_\"+model_name+\"_\"+name+\".json\"\n",
    "            \n",
    "            with open(path,\"w\",encoding=\"utf-8\") as f:\n",
    "                json.dump(embs[i],f)\n",
    "    \n",
    "model_lists = [\"dpr\",\"dpr-multiset\"]\n",
    "model_lists.extend([\"simcse-unsup\",\"simcse-sup\",\"simcse-unsup-large\",\"simcse-sup-large\",\"simcse-unsup-rb\",\"simcse-sup-rb\",\"simcse-unsup-large-rb\",\"simcse-sup-large-rb\"])\n",
    "\n",
    "\n",
    "for model_name in model_lists:\n",
    "    print(model_name)\n",
    "    extract_embeddings(datasets, model_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6d78f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the computation below is for demo-purpose\n",
    "\n",
    "def general_pir_main(datasets, model_name=\"simcse-sup\",mode=\"vec_cast\"):\n",
    "\n",
    "    for k,v in datasets.items():\n",
    "        \n",
    "        print(\"we are working on:\",k)\n",
    "        \n",
    "        corpus_scores = []\n",
    "\n",
    "        queries = v[\"queries\"]\n",
    "        query_labels = v[\"query_labels\"]\n",
    "        corpus = v[\"corpus\"]\n",
    "        key_ref = v[\"key_ref\"]\n",
    "        perspectives = v[\"perspectives\"]\n",
    "        \n",
    "        if mode == \"bm25_ranking\":\n",
    "            tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "            bm25 = BM25Okapi(tokenized_corpus) \n",
    "        \n",
    "        embs_collection = {}\n",
    "        \n",
    "        for name in [\"queries\",\"source_queries\",\"perspectives\",\"corpus\"]:\n",
    "            with open(\"./embs/\"+k+\"_\"+model_name+\"_\"+name+\".json\",\"r\",encoding=\"utf-8\") as f:\n",
    "                embs_collection[name] = json.load(f)\n",
    "            \n",
    "        query_embeddings = embs_collection[\"queries\"]\n",
    "        source_query_embeddings = embs_collection[\"source_queries\"]\n",
    "        perspectives_embeddings = embs_collection[\"perspectives\"]\n",
    "        corpus_embeddings = embs_collection[\"corpus\"]\n",
    "        \n",
    "\n",
    "        for index in trange(len(query_embeddings)):\n",
    "            emb_q = query_embeddings[index]\n",
    "            emb_s = source_query_embeddings[index]\n",
    "            emb_p = perspectives_embeddings[index]\n",
    "            \n",
    "            scores = []\n",
    "            bm25_scores = []\n",
    "            p_scores = []\n",
    "            \n",
    "            for i, emb_c in enumerate(corpus_embeddings):\n",
    "                \n",
    "                # vector manipulation: aug denotes using q, instead of s\n",
    "                if mode == \"vec_projection_rev\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    if k == \"ambigqa\":\n",
    "                        emb_p = emb_q - emb_s\n",
    "                    weight = np.dot(emb_q, emb_p)/np.dot(emb_p, emb_p)\n",
    "                    context_score = 1 - cosine(emb_q + weight*emb_p, emb_c)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                if mode == \"vec_dual_projection_rev\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    if k == \"ambigqa\":\n",
    "                        emb_p = emb_q - emb_s\n",
    "                    p_cor = np.dot(emb_p, emb_p)\n",
    "                    weight_q = np.dot(emb_q, emb_p)/p_cor\n",
    "                    weight_c = np.dot(emb_c, emb_p)/p_cor\n",
    "                    context_score = 1 - cosine(emb_q + weight_q*emb_p, emb_c + weight_c*emb_p)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                if mode == \"vec_projection\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    if k == \"ambigqa\":\n",
    "                        emb_p = emb_q - emb_s\n",
    "                    weight = np.dot(emb_q, emb_p)/np.dot(emb_p, emb_p)\n",
    "                    context_score = 1 - cosine(emb_q - weight*emb_p, emb_c)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                if mode == \"vec_dual_projection\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    if k == \"ambigqa\":\n",
    "                        emb_p = emb_q - emb_s\n",
    "                    p_cor = np.dot(emb_p, emb_p)\n",
    "                    weight_q = np.dot(emb_q, emb_p)/p_cor\n",
    "                    weight_c = np.dot(emb_c, emb_p)/p_cor\n",
    "                    context_score = 1 - cosine(emb_q - weight_q*emb_p, emb_c- weight_c*emb_p)\n",
    "                    scores.append(context_score)\n",
    "                \n",
    "                if mode == \"vec_add\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    context_score = 1 - cosine(emb_s+emb_p, emb_c)\n",
    "                    scores.append(context_score)\n",
    "\n",
    "                if mode == \"vec_concat\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    emb_sp = np.concatenate((emb_s, emb_p), axis=None)\n",
    "                    emb_cp = np.concatenate((emb_c, emb_p), axis=None)\n",
    "                    context_score = 1 - cosine(emb_sp, emb_cp)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                if mode == \"vec_aug_add\":\n",
    "                    emb_q,emb_c,emb_p = np.array(emb_q),np.array(emb_c),np.array(emb_p)\n",
    "                    context_score = 1 - cosine(emb_q+emb_p, emb_c)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                if mode == \"vec_aug_concat\":\n",
    "                    emb_q,emb_c,emb_p = np.array(emb_q),np.array(emb_c),np.array(emb_p)\n",
    "                    emb_qp = np.concatenate((emb_q, emb_p), axis=None)\n",
    "                    emb_cp = np.concatenate((emb_c, emb_p), axis=None)\n",
    "                    context_score = 1 - cosine(emb_qp, emb_cp)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                if mode == \"vec_dual_concat\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    emb_sp = np.concatenate((emb_s, emb_p), axis=None)\n",
    "                    emb_cp = np.concatenate((emb_c, emb_p), axis=None)\n",
    "                    context_score = 1 - cosine(emb_sp, emb_cp)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                if mode == \"vec_cast_single\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    if k == \"ambigqa\":\n",
    "                        emb_p = emb_q - emb_s\n",
    "                    context_score = 1 - cosine(emb_s-emb_p, emb_c)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                if mode == \"vec_aug_cast_single\":\n",
    "                    emb_q,emb_c,emb_p = np.array(emb_q),np.array(emb_c),np.array(emb_p)\n",
    "                    if k == \"ambigqa\":\n",
    "                        emb_p = emb_q - emb_s\n",
    "                    context_score = 1 - cosine(emb_q-emb_p, emb_c)\n",
    "                    scores.append(context_score)     \n",
    "                    \n",
    "                if mode == \"vec_cast\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    if k == \"ambigqa\":\n",
    "                        emb_p = emb_q - emb_s\n",
    "                    context_score = 1 - cosine(emb_s-emb_p, emb_c-emb_p)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                if mode == \"vec_aug_cast\":\n",
    "                    emb_q,emb_c,emb_p = np.array(emb_q),np.array(emb_c),np.array(emb_p)\n",
    "                    if k == \"ambigqa\":\n",
    "                        emb_p = emb_q - emb_s\n",
    "                    context_score = 1 - cosine(emb_q-emb_p, emb_c-emb_p)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                    \n",
    "                # score manipulation\n",
    "                if mode == \"additive\":\n",
    "                    context_score = 1 - cosine(emb_s, emb_c)\n",
    "                    perspective_score = 1 - cosine(emb_p, emb_c)\n",
    "                    scores.append(context_score + perspective_score)\n",
    "                    \n",
    "                if mode == \"additive_aug\":\n",
    "                    context_score = 1 - cosine(emb_q, emb_c)\n",
    "                    perspective_score = 1 - cosine(emb_p, emb_c)\n",
    "                    scores.append(context_score + perspective_score)\n",
    "                    \n",
    "                if mode == \"additive_tripple\":\n",
    "                    source_score = 1 - cosine(emb_s, emb_c)\n",
    "                    context_score = 1 - cosine(emb_q, emb_c)\n",
    "                    perspective_score = 1 - cosine(emb_p, emb_c)\n",
    "                    scores.append(source_score + context_score + perspective_score)\n",
    "                    \n",
    "                \n",
    "                # re-ranking\n",
    "                if mode == \"re-ranking\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    q_score = 1 - cosine(emb_s, emb_c)\n",
    "                    context_score = 1 - cosine(emb_p, emb_c)\n",
    "                    scores.append(q_score)\n",
    "                    p_scores.append(context_score)\n",
    "\n",
    "                if mode == \"re-ranking_aug\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    q_score = 1 - cosine(emb_q, emb_c)\n",
    "                    context_score = 1 - cosine(emb_p, emb_c)\n",
    "                    scores.append(q_score)\n",
    "                    p_scores.append(context_score)\n",
    "                \n",
    "                if mode == \"bm25_ranking\":\n",
    "                    query = queries[index]\n",
    "                    context = corpus[index]\n",
    "                    kws = perspectives[index].split(\" \")\n",
    "\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    context_score = 1 - cosine(emb_s-emb_p, emb_c-emb_p)\n",
    "                    scores.append(context_score)\n",
    "\n",
    "                    tokenized_query = query.split(\" \")\n",
    "                    doc_scores = bm25.get_scores(tokenized_query)\n",
    "                    bm25_scores.append(doc_scores[i])\n",
    "            \n",
    "            if mode == \"re-ranking\" or mode == \"re-ranking_aug\":\n",
    "                thresh = float(np.percentile(np.array(scores), 80))\n",
    "                temp_scores = []\n",
    "                for i, score in enumerate(scores):\n",
    "                    if score >= thresh:\n",
    "                        temp_score = score + p_scores[i]\n",
    "                    else:\n",
    "                        temp_score = -100\n",
    "                scores = temp_scores\n",
    "            \n",
    "            if mode == \"bm25_ranking\":\n",
    "                sm = statistics.mean(scores)\n",
    "                bm25m = statistics.mean(bm25_scores)\n",
    "                temp_scores = []\n",
    "                for i, score in enumerate(scores):\n",
    "                    temp_scores = score-sm+bm25_scores[i]-bm25m\n",
    "                scores = temp_scores\n",
    "                    \n",
    "            corpus_scores.append(scores)\n",
    "            \n",
    "        with open(\"./pir_scores/pir_\"+k+\"_\"+mode+\"_scores.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "            json.dump(corpus_scores,f)\n",
    "            \n",
    "        evaluation(key_ref, corpus_scores, query_labels, k)\n",
    "        print()\n",
    "\n",
    "all_pir_mode = []\n",
    "\n",
    "# all_pir_mode.extend([\"vec_add\",\"vec_concat\",\"vec_aug_add\",\"vec_aug_concat\",\"vec_dual_concat\",\"vec_cast\",\"vec_aug_cast\"])\n",
    "# all_pir_mode.extend([\"additive\",\"additive_aug\",\"additive_tripple\"])\n",
    "# all_pir_mode.extend([\"vec_cast_single\",\"vec_aug_cast_single\", \"re-ranking\", \"re-ranking_aug\",\"bm25_ranking\"])\n",
    "\n",
    "# all_pir_mode.extend([\"vec_projection\",\"vec_dual_projection\",\"vec_cast\",\"vec_aug_cast\"])\n",
    "\n",
    "# all_pir_mode.extend([\"vec_projection_rev\",\"vec_dual_projection_rev\"])\n",
    "# for mode in all_pir_mode:\n",
    "#     print(\"-------------------\",mode,\"--------------------------\")\n",
    "#     general_pir_main(datasets,model_name=\"simcse-sup\",mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddb2112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the PIR scores computed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
